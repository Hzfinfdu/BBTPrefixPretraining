-----------------------------------------
Dataset [nlpcc_dbqaDataset] with task id [21].
An example: [意 思 判 别 ： " 小 红 参 有 什 么 功 能 ？ " 与 " 花 期 7～8 月 ， 果 期 8～10 月 。 " 的 关 系 是 ？ 选 项 ： 矛 盾 ， 蕴 含 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [矛 盾]
-----------------------------------------

-----------------------------------------
Dataset [ClueWSCDataset] with task id [13].
An example: [指 代 判 别 : 在 句 子 " 但 我 注 意 到 画 里 有 一 样 东 西 ， 那 是 一 座 水 塔 ， 与 周 围 的 旧 楼 相 比 它 的 色 彩 稍 微 艳 丽 了 一 些 ， 想 一 朵 高 大 的 喇 叭 花 。 " 中 ， 代 词 " 它 " 指 代 的 和 " 旧 楼 " 一 致 吗 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 同]
-----------------------------------------

-----------------------------------------
Dataset [LawzhidaoDataset] with task id [31].
An example: [问 答 判 断 ： " 男 子 离 婚 后 拒 付 孩 子 抚 养 费 称 [UNK] 改 名 换 姓 了 怎 么 付 [UNK] " 的 回 答 " 子 女 改 名 字 与 付 抚 养 费 没 关 系 ， 抚 养 费 是 法 律 支 持 的 范 围 ， 可 以 经 法 院 诉 讼 后 ， 强 制 性 给 付 承 担 " 符 合 问 题 吗 ？ 选 项 ： 符 合 ， 不 符 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [符 合]
-----------------------------------------

-----------------------------------------
Dataset [LawzhidaoDataset] with task id [31].
An example: [问 答 判 断 ： " 婚 前 房 产 离 婚 后 其 中 一 方 过 户 给 另 一 方 请 问 需 要 " 的 回 答 " 离 婚 时 ， 一 方 将 婚 前 房 产 给 另 一 方 ， 过 户 时 不 需 要 交 税 。 可 以 申 请 办 理 析 产 过 户 ， 只 要 几 百 元 手 续 费 。 具 体 手 续 办 理 ： 本 人 与 房 屋 所 有 权 人 （ 原 配 偶 ） 到 原 房 证 办 理 部 门 办 理 离 婚 析 产 登 记 。 要 件 如 下 ： 1. 登 记 申 请 书 （ 原 件 ） ； 2. 申 请 人 身 份 证 明 （ 核 实 原 件 留 存 复 印 件 ） ； 3. 房 屋 所 有 权 证 （ 原 件 ） ； 4. 经 民 政 局 协 议 离 婚 的 提 供 离 婚 证 和 离 婚 协 议 书 （ 核 实 原 件 留 存 复 印 件 ） ； 5. 完 税 证 明 （ 核 实 原 件 留 存 复 印 件 ） ； 6. 其 他 必 要 材 料 。 兰 * 律 师 ： 网 页 链 接 要 求 是 双 方 一 起 到 房 管 部 门 办 理 ， 需 要 住 房 贷 款 还 清 ， 无 抵 押 ， 有 房 产 证 ， 契 税 证 ， 土 地 证 。 " 符 合 问 题 吗 ？ 选 项 ： 符 合 ， 不 符 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 符]
-----------------------------------------

-----------------------------------------
Dataset [CSLDataset] with task id [3].
An example: [主 题 判 别 : " 石 墨 烯 是 一 种 具 有 大 比 表 面 积 、 高 电 导 率 和 良 好 的 力 学 性 能 的 二 维 材 料, 在 高 容 量 和 大 功 率 储 能 器 件 方 面 具 有 广 阔 的 应 用 前 景. 然 而 现 有 的 各 种 石 墨 烯 电 极 制 造 技 术 无 论 从 技 术 层 面 还 是 在 生 产 率 、 性 能 方 面 都 难 以 满 足 当 前 工 业 应 用 的 需 求. 石 墨 烯 增 材 制 造 ( 石 墨 烯 3d 打 印 ) 在 复 杂 三 维 石 墨 烯 结 构 的 制 造 方 面 具 有 突 出 的 优 势 和 潜 力, 而 且 还 具 有 设 备 简 单 、 成 型 结 构 可 控 性 高 等 优 点. 关 于 石 墨 烯 基 电 极 材 料 的 增 材 制 造 及 应 用 在 近 两 年 内 迅 速 发 展. 概 述 了 基 于 增 材 制 造 制 备 石 墨 烯 结 构 的 典 型 技 术 [UNK] [UNK] 直 写 成 型 ( diw ) 的 机 理 和 优 点, 介 绍 了 基 于 该 技 术 制 备 的 石 墨 烯 基 电 极 材 料 在 超 级 电 容 器 和 锂 离 子 电 池 领 域 的 应 用, 最 后 对 石 墨 烯 基 电 极 材 料 的 增 材 制 造 面 临 的 挑 战 和 未 来 发 展 趋 势 进 行 了 展 望. " 和 关 键 词 " ['电 极 材 料 ','增 材 制 造 ','材 料'] " 一 致 吗 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 同]
-----------------------------------------

-----------------------------------------
Dataset [OnlineShppingDataset] with task id [34].
An example: [情 感 分 析 ： " 2699 ， 价 格 无 敌 ， 比 电 脑 卖 我 问 的 所 谓 最 低 价 场 足 足 便 宜 了 300 ， 性 能 也 很 不 错 ， 待 机 时 间 正 在 测 试 中 ， 不 可 能 和 官 方 9. 5 小 时 一 样 ， 那 太 理 想 化 了 ， 不 过 估 计 7 ， 8 个 小 时 问 题 不 大 " 的 情 感 是 ？ 选 项 ： 负 面 ， 正 面 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [正 面]
-----------------------------------------

-----------------------------------------
Dataset [ChipStsDataset] with task id [11].
An example: [意 思 判 别 ： " 高 血 糖 高 血 压 高 血 脂 能 吃 核 桃 吗 ？ " 与 " 杏 仁 茶 对 高 血 压 的 影 响 ？ " 的 意 思 是 ？ 选 项 ： 不 同 ， 相 似 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [相 似]
-----------------------------------------

-----------------------------------------
Dataset [KUAKE_QQRDataset] with task id [16].
An example: [意 思 判 别 ： " 吃 了 胶 水 丝 " 与 " 喝 了 胶 水 怎 么 办 " 的 关 系 是 ？ 选 项 ： 矛 盾 ， 中 立 ， 蕴 含 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [中 立]
-----------------------------------------

-----------------------------------------
Dataset [Cmrc2018Dataset] with task id [12].
An example: [抽 取 式 问 答 ： 文 本 " ba - 6 （ ） 是 苏 联 在 1930 年 代 早 期 研 制 的 装 甲 车 。 ba - 6 使 用 的 45 毫 米 炮 （ bt 炮 塔 ） 和 t - 26 坦 克 一 样 ， 另 装 备 1 挺 7. 62 毫 米 dt 机 枪 于 车 身 中 部 ， 一 挺 dt 同 轴 机 枪 于 炮 塔 。 ba - 6 重 5. 1 吨 ， 车 长 4. 65 米 ， 宽 2. 1 米 ， 高 2. 2 米 ， 乘 员 4 名 。 ba - 6 使 用 美 国 制 gaz - aaa 底 盘 ， 这 种 底 盘 只 适 合 在 公 路 等 良 好 路 况 下 行 驶 。 这 情 况 后 来 在 增 加 一 对 适 合 路 轮 时 得 到 一 点 改 善 。 ba - 6 外 形 接 近 ba - 3 ， 但 ba - 6 取 消 了 右 后 方 一 扇 车 门 。 ba - 3 装 甲 过 于 笨 重 ， 而 ba - 6 装 甲 较 薄 （ 10 毫 米 ） 且 性 能 更 好 。 ba - 6 最 后 被 更 佳 的 ba - 10 取 代 。 在 30 年 代 早 期 ， ba - 6 等 装 甲 车 可 轻 易 击 毁 大 部 份 同 期 装 甲 车 ， 但 其 薄 薄 的 装 甲 还 是 容 易 被 小 口 径 炮 击 破 。 ba - 6 参 加 了 早 期 东 线 的 战 斗 。 由 于 其 装 甲 不 足 以 应 付 德 军 火 力 ， 所 以 其 侦 察 角 色 被 t - 60 坦 克 及 t - 70 坦 克 等 取 代 。 问 题 " ba - 6 在 外 型 上 有 什 么 特 点 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [重 5. 1 吨 ， 车 长 4. 65 米 ， 宽 2. 1 米 ， 高 2. 2 米 ， 乘 员 4 名 。]
-----------------------------------------

-----------------------------------------
Dataset [DMSCDataset] with task id [29].
An example: [打 分 ： " 怒 赞 音 乐 ， 在 我 心 中 ， 音 乐 好 的 电 影 都 差 不 到 哪 去 ， 至 少 很 大 程 度 上 证 明 了 导 演 的 品 味 和 风 格 。 作 为 一 部 导 演 处 女 作 来 说 ， 各 方 面 都 达 到 良 好 ， 经 典 台 词 到 处 都 是 ， 不 只 是 段 子 ， 值 得 回 味 。 [UNK] 喜 欢 就 会 放 肆 ， 但 爱 是 克 制 。 [UNK] [UNK] 小 孩 才 分 对 错 ， 大 人 只 看 利 弊 。 [UNK] 后 会 无 期 。 " 的 评 价 是 ？ 选 项 ： 非 常 差 ， 较 差 ， 一 般 ， 较 好 ， 非 常 好 ，]
Its label is [较 好]
-----------------------------------------

-----------------------------------------
Dataset [toutiao_tcDataset] with task id [19].
An example: [主 题 识 别 ： " 北 京 交 通 大 学 [UNK] 一 带 一 路 [UNK] 国 际 人 才 联 合 培 养 肯 尼 亚 项 目 第 三 批 留 学 生 开 学 典 礼 在 威 海 校 区 举 行 ; 威 海 校 区 ; 肯 尼 亚 ; 北 京 交 通 大 学 ; 中 非 ; sisi ni moja ; 中 国 路 桥 ; 茉 莉 花 " 的 主 题 是 ？ 选 项 ： 房 产 ， 汽 车 ， 金 融 ， 体 育 ， 文 化 ， 娱 乐 ， 教 育 ， 科 技 ， 军 事 ， 旅 游 ， 世 界 ， 农 业 ， 股 票 ， 游 戏 ， 故 事 ，]
Its label is [教 育]
-----------------------------------------

-----------------------------------------
Dataset [SanWenDataset] with task id [18].
An example: [关 系 判 别 ： 主 语 " 自 己 " 和 宾 语 " 自 己 的 文 学 作 品 " 在 句 子 " 自 己 的 文 学 作 品 必 须 扎 根 在 自 己 民 族 的 土 壤 里 ， 自 己 的 文 学 作 品 必 须 描 写 自 己 民 族 的 风 土 人 情 " 中 的 关 系 是 ？ 选 项 ： 未 知 ， 创 建 ， 使 用 ， 贴 近 ， 位 于 ， 占 有 ， 社 会 相 关 ， 家 庭 关 系 ， 一 般 与 特 别 ， 部 分 与 整 体 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [创 建]
-----------------------------------------

-----------------------------------------
Dataset [CoteDpDataset] with task id [22].
An example: [抽 取 式 问 答 ： 文 本 " 感 觉 自 己 真 的 是 挺 乡 巴 佬 的 ， 居 然 是 第 一 次 来 到 福 州 路 店 的 川 锅 一 号 吃 火 锅 。 问 题 " 这 句 话 的 中 心 是 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [川 锅 一 号]
-----------------------------------------

-----------------------------------------
Dataset [nlpcc_tcDataset] with task id [17].
An example: [情 感 识 别 ： " 听 不 懂 白 买 了 。 " 的 主 题 是 ？ 选 项 ： 负 面 ， 正 面 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [负 面]
-----------------------------------------

-----------------------------------------
Dataset [CCPMDataset] with task id [24].
An example: [诗 句 理 解 ： 与 句 子 " 喝 醉 酒 酣 睡 不 知 天 已 黄 昏 。 " 最 相 近 的 诗 句 是 ？ 选 项 ： 醉 卧 不 知 白 日 暮 ， 醉 吟 终 日 不 知 老 ， 啼 鸟 不 知 白 日 午 ， 真 游 不 知 白 日 晚 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [醉 卧 不 知 白 日 暮]
-----------------------------------------

-----------------------------------------
Dataset [ClueWSCDataset] with task id [13].
An example: [指 代 判 别 : 在 句 子 " 他 伯 父 还 有 许 多 女 弟 子 ， 大 半 是 富 商 财 主 的 外 室 ； 这 些 财 翁 白 天 忙 着 赚 钱 ， 怕 小 公 馆 里 的 情 妇 长 日 无 聊 ， 要 不 安 分 ， 常 常 叫 她 们 学 点 玩 艺 儿 消 遣 。 " 中 ， 代 词 " 她 们 " 指 代 的 和 " 他 伯 父 " 一 致 吗 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 同]
-----------------------------------------

-----------------------------------------
Dataset [TouzizhidaoDataset] with task id [35].
An example: [问 答 判 断 ： " 需 要 具 备 什 么 条 件 才 能 办 信 用 卡 ， 刚 开 始 能 透 支 一 万 吗 " 的 回 答 " 刚 开 始 不 能 透 支 一 万 一 ， 申 请 条 件 ： 1 ， 年 满 18 周 岁 至 60 周 岁 ， 具 有 完 全 民 事 行 为 能 力 ； 2 ， 有 良 好 信 用 记 录 ； 3 ， 有 工 作 收 入 且 具 备 还 款 能 力 。 二 ， 申 请 材 料 ： 1 ， 身 份 证 明 资 料 ： 居 民 身 份 证 或 者 军 官 证 复 印 件 ； 2 ， 工 作 及 收 入 证 明 资 料 ； 3 ， 其 他 财 力 证 明 ： 如 房 产 证 复 印 件 、 汽 车 行 驶 证 复 印 件 、 存 单 复 印 件 等 。 三 ， 信 用 卡 申 请 方 法 ： 1 ， 柜 台 申 请 ： 携 带 资 信 证 明 到 柜 面 申 请 ： 需 要 申 请 人 提 交 比 较 完 备 的 资 产 证 明 文 件 ， 向 银 行 提 交 材 料 供 银 行 后 台 审 核 部 门 审 核 ； 审 核 通 过 后 ， 填 写 相 关 资 料 ， 身 份 信 息 ； 发 放 信 用 卡 。 2 ， 网 银 申 请 ： 申 请 人 用 个 人 名 下 储 蓄 卡 账 户 开 通 网 银 ， 进 入 银 行 官 网 ； 输 入 卡 号 、 登 陆 密 码 和 验 证 码 后 进 入 网 银 页 面 ； 点 击 [UNK] 信 用 卡 服 务 [UNK] ， 可 在 信 用 卡 服 务 页 面 下 选 择 申 请 信 用 卡 ， 填 写 资 料 后 提 交 ， 审 核 进 度 、 结 果 在 网 银 上 显 示 ； 审 核 通 过 后 ， 发 放 信 用 卡 。 3 ， 柜 台 办 理 流 程 ： 到 办 理 的 信 用 卡 的 银 行 网 点 ， 或 者 找 当 地 信 用 卡 中 心 工 作 人 员 ， 填 写 信 用 卡 申 请 表 。 申 请 表 的 内 容 一 般 包 括 申 领 人 的 名 称 、 基 本 情 况 、 经 济 状 况 或 收 入 符 合 ， 不 符 ，]
Its label is [符 合]
-----------------------------------------

-----------------------------------------
Dataset [BQDataset] with task id [5].
An example: [意 思 判 别 ： " 刚 刚 你 们 打 电 话 来 ， 我 没 听 到 ， 现 在 可 以 再 打 一 次 " 与 " 刚 才 忙 去 了 没 听 到 ， 现 在 可 以 打 电 话 审 核 了 " 的 意 思 是 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD]]
Its label is [相 同]
-----------------------------------------

-----------------------------------------
Dataset [xnliDataset] with task id [20].
An example: [意 思 判 别 ： " 巴 黎 或 纽 约 不 是, 但 它 的 市 区 街 道 在 几 十 年 前 就 有 了 polyglot 的 活 力. " 与 " 该 城 市 缺 乏 多 样 性 和 文 化. " 的 关 系 是 ？ 选 项 ： 矛 盾 ， 中 立 ， 蕴 含 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [矛 盾]
-----------------------------------------

-----------------------------------------
Dataset [PawsDataset] with task id [1].
An example: [意 思 判 别 ： " 这 种 分 歧 发 生 或 触 及 西 弗 吉 尼 亚 州 ， 弗 吉 尼 亚 州 ， 北 卡 罗 来 纳 州 ， 南 卡 罗 来 纳 州 ， 佐 治 亚 州 ， 阿 拉 巴 马 州 ， 密 西 西 比 州 ， 田 纳 西 州 和 肯 塔 基 州 。 " 与 " 分 歧 通 过 或 触 及 西 弗 吉 尼 亚 州 ， 弗 吉 尼 亚 州 ， 北 卡 罗 来 纳 州 ， 南 卡 罗 来 纳 州 ， 佐 治 亚 州 ， 阿 拉 巴 马 州 ， 密 西 西 比 州 ， 田 纳 西 州 和 肯 塔 基 州 。 " 的 关 系 是 ？ 选 项 ： 矛 盾 ， 中 立 ， 蕴 含 ，]
Its label is [中 立]
-----------------------------------------

-----------------------------------------
Dataset [DuReaderRobustDataset] with task id [10].
An example: [抽 取 式 问 答 ： 文 本 " a. m. = a. m. = amante meridiem ( before noon ) 上 午 ， 午 前 p. m = p. m. post meridiem ( = afternoon ) 下 午 ， 午 后 pm 是 错 的 ， 1 、 圣 皮 埃 尔 和 密 克 隆 群 岛 2 、 [UNK] 3 、 首 相 ； 总 理 ( prime minister ) | a. m. = a. m. = amante meridiem ( before noon ) 上 午 ， 午 前 问 题 " 早 上 英 文 缩 写 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [a. m]
-----------------------------------------

-----------------------------------------
Dataset [DMSCDataset] with task id [29].
An example: [打 分 ： " 流 水 账 没 主 题 。 吴 亦 凡 演 技 没 有 ， 情 绪 靠 吼 。 " 的 评 价 是 ？ 选 项 ： 非 常 差 ， 较 差 ， 一 般 ， 较 好 ， 非 常 好 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [非 常 差]
-----------------------------------------

-----------------------------------------
Dataset [ClueWSCDataset] with task id [13].
An example: [指 代 判 别 : 在 句 子 " 我 哥 哥 升 入 高 中 没 多 久 ， 开 始 结 交 城 里 同 学 。 与 此 同 时 ， 他 对 村 中 孩 子 的 态 度 变 得 越 来 越 冷 漠 。 " 中 ， 代 词 " 他 " 指 代 的 和 " 城 里 同 学 " 一 致 吗 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 同]
-----------------------------------------

-----------------------------------------
Dataset [CCPMDataset] with task id [24].
An example: [诗 句 理 解 ： 与 句 子 " 人 生 百 年 不 必 飘 泊 承 受 无 尽 的 苦 难 ， 我 想 就 在 这 里 安 度 余 年 ， 从 从 容 容 。 " 最 相 近 的 诗 句 是 ？ 选 项 ： 百 岁 身 躯 浪 得 名 ， 提 携 何 苦 自 劳 形 。 ， 百 年 形 影 浪 自 苦 ， 便 欲 此 地 安 微 躬 。 ， 十 日 羁 穷 徒 自 苦 ， 百 年 生 事 欲 谁 论 。 ， 逐 妄 百 年 真 自 苦 ， 造 微 一 日 有 馀 功 。 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [百 年 形 影 浪 自 苦 ， 便 欲 此 地 安 微 躬 。]
-----------------------------------------

-----------------------------------------
Dataset [nlpcc_tcDataset] with task id [17].
An example: [情 感 识 别 ： " 不 喜 欢 小 美 人 鱼 2 ， 没 有 1 好 看 ~ 2 的 任 务 形 象 太 没 创 意 了 ， 特 别 不 喜 欢 她 的 头 发 变 成 黑 色 （ 当 然 我 绝 对 不 是 讨 厌 黑 色 ， 就 当 前 人 物 形 象 而 言 的 ） 了 ， 感 觉 好 邪 恶 的 ~ ~ ！ 不 过 版 本 质 量 还 是 不 错 的 ~ " 的 主 题 是 ？ 选 项 ： 负 面 ， 正 面 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [负 面]
-----------------------------------------

-----------------------------------------
Dataset [ChipCtcDataset] with task id [6].
An example: [主 题 识 别 ： " 1 ) hiv 抗 体 阳 性 或 可 疑 ， hiv 核 酸 阳 性 ； " 的 主 题 是 ？ 选 项 ： 疾 病 ， 症 状 ， 迹 象 ， 孕 期 ， 肿 瘤 ， 过 敏 ， 口 腔 ， 药 学 ， 疗 法 ， 设 备 ， 护 理 ， 诊 断 ， 年 龄 ， 性 别 ， 教 育 ， 地 址 ， 种 族 ， 意 愿 ， 容 量 ， 伦 理 ， 睡 眠 ， 运 动 ， 饮 食 ， 吸 烟 ， 献 血 ， 就 医 ， 残 障 ， 健 康 ， 数 据 ， 综 合 ， 饮 酒 者 ， 性 相 关 ， 符 合 协 议 ， 成 瘾 行 为 ， 器 官 组 织 ， 预 期 寿 命 ， 风 险 评 估 ， 受 体 状 态 ， 病 情 发 展 ， 特 殊 体 征 ， 专 业 知 识 ， 实 验 室 检 查 ， 研 究 者 决 策 ， 参 与 其 他 研 究 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [实 验 室 检 查]
-----------------------------------------

-----------------------------------------
Dataset [ChipStsDataset] with task id [11].
An example: [意 思 判 别 ： " 高 血 压 孕 妇 高 压 130 低 压 100 ） 可 以 顺 产 吗 " 与 " 如 果 是 高 血 压 怎 么 办 可 以 顺 产 吗 " 的 意 思 是 ？ 选 项 ： 不 同 ， 相 似 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [相 似]
-----------------------------------------

-----------------------------------------
Dataset [xnliDataset] with task id [20].
An example: [意 思 判 别 ： " 圣 殿 的 遗 址 最 终 被 确 定 为 mt. 亚 伯 拉 罕 被 召 唤 去 牺 牲 他 的 儿 子 艾 萨 克 " 与 " 这 完 全 是 根 据 传 说, 因 为 有 几 位 专 家 相 信 这 一 切 都 发 生 在 另 一 个 地 点. " 的 关 系 是 ？ 选 项 ： 矛 盾 ， 中 立 ， 蕴 含 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [中 立]
-----------------------------------------

-----------------------------------------
Dataset [BQDataset] with task id [5].
An example: [意 思 判 别 ： " 其 实 我 的 问 题 很 简 单 ， 我 想 获 得 微 粒 贷 的 体 验 资 格 ， 可 是 等 了 很 久 了 都 没 等 到'" 与 " 借 钱 在 哪 里 " 的 意 思 是 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 同]
-----------------------------------------

-----------------------------------------
Dataset [OnlineShppingDataset] with task id [34].
An example: [情 感 分 析 ： " 酒 店 位 置 不 错, 在 市 中 心, 服 务 态 度 很 好, 设 施 还 可 以. 只 是 床 太 硬. " 的 情 感 是 ？ 选 项 ： 负 面 ， 正 面 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [正 面]
-----------------------------------------

-----------------------------------------
Dataset [Cmrc2018Dataset] with task id [12].
An example: [抽 取 式 问 答 ： 文 本 " 白 苞 蒿 （ 学 名 ： ） 是 菊 科 蒿 属 的 植 物 。 分 布 在 新 加 坡 、 老 挝 、 印 度 、 柬 埔 寨 、 越 南 、 印 度 尼 西 亚 、 台 湾 岛 以 及 中 国 大 陆 的 江 苏 、 湖 南 、 贵 州 、 湖 北 、 广 西 、 广 东 、 浙 江 、 甘 肃 、 云 南 、 四 川 、 福 建 、 安 徽 、 江 西 、 陕 西 、 河 南 等 地 ， 生 长 于 海 拔 3, 000 米 的 地 区 ， 多 生 于 山 谷 等 湿 润 、 林 下 、 林 缘 、 灌 丛 边 缘 以 及 略 为 干 燥 地 区 。 秦 州 庵 闾 子 （ 政 和 本 草 ） ， 鸭 脚 艾 （ 生 草 药 性 备 要 ） ， 鸡 甜 菜 （ 陆 川 本 草 ） ， 四 季 菜 （ 江 苏 南 部 种 子 植 物 手 册 ） ， 白 花 蒿 （ 海 南 植 物 志 ） ， 广 东 刘 寄 奴 、 甜 菜 子 （ 福 建 、 浙 江 、 广 东 、 广 西 ） ， 野 芹 菜 、 白 花 艾 、 鸭 脚 菜 （ 湖 南 、 广 西 ） ， 珍 珠 菊 （ 江 西 ） ， 土 三 七 、 肺 痨 草 、 野 红 芹 菜 、 白 米 蒿 （ 四 川 ） ， 红 姨 妈 菜 （ 贵 州 ）, 角 菜 、 珍 珠 菜 、 真 珠 菜 、 香 芹 菜 、 四 季 菜 、 乳 白 艾 、 甜 菜 、 香 甜 菜 、 纳 艾 香 、 鸭 掌 艾 问 题 " 在 中 国 ， 白 苞 蒿 分 布 在 哪 些 省 市 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [中 国 大 陆 的 江 苏 、 湖 南 、 贵 州 、 湖 北 、 广 西 、 广 东 、 浙 江 、 甘 肃 、 云 南 、 四 川 、 福 建 、 安 徽 、 江 西 、 陕 西 、 河 南 等 地]
-----------------------------------------

-----------------------------------------
Dataset [Cmrc2018Dataset] with task id [12].
An example: [抽 取 式 问 答 ： 文 本 " 徐 珂 （ ） ， 原 名 昌 ， 字 仲 可 ， 浙 江 杭 县 （ 今 属 杭 州 市 ） 人 。 光 绪 举 人 。 后 任 商 务 印 书 馆 编 辑 。 参 加 南 社 。 1901 年 在 上 海 担 任 了 《 外 交 报 》 、 《 东 方 杂 志 》 的 编 辑 ， 1911 年 ， 接 管 《 东 方 杂 志 》 的 [UNK] 杂 纂 部 [UNK] 。 与 潘 仕 成 、 王 晋 卿 、 王 辑 塘 、 冒 鹤 亭 等 友 好 。 编 有 《 清 稗 类 钞 》 、 《 历 代 白 话 诗 选 》 、 《 古 今 词 选 集 评 》 等 。 光 绪 十 五 年 （ 1889 年 ） 举 人 。 后 任 商 务 印 书 馆 编 辑 。 参 加 南 社 。 曾 担 任 袁 世 凯 在 天 津 小 站 练 兵 时 的 幕 僚 ， 不 久 离 去 。 问 题 " 1911 年 ， 徐 珂 担 任 什 么 职 务 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [1911 年 ， 接 管 《 东 方 杂 志 》 的 [UNK] 杂 纂 部 [UNK] 。]
-----------------------------------------

-----------------------------------------
Dataset [CoteDpDataset] with task id [22].
An example: [抽 取 式 问 答 ： 文 本 " 五 芳 斋 平 时 真 的 很 少 去 那 里 吃 东 西 ， 现 在 已 经 越 来 越 多 元 化 了 ， 饭 菜 面 各 色 都 有 的 。 问 题 " 这 句 话 的 中 心 是 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [五 芳 斋]
-----------------------------------------

-----------------------------------------
Dataset [ClueWSCDataset] with task id [13].
An example: [指 代 判 别 : 在 句 子 " 老 白 又 引 客 人 进 来 ， 爱 默 起 身 招 待 ， 心 还 逗 留 在 这 长 得 聪 明 的 孩 子 身 上 ， 想 他 该 是 受 情 感 教 育 的 年 纪 了 。 " 中 ， 代 词 " 他 " 指 代 的 和 " 客 人 " 一 致 吗 ？ 选 项 ： 不 同 ， 相 同 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [不 同]
-----------------------------------------

-----------------------------------------
Dataset [OnlineShppingDataset] with task id [34].
An example: [情 感 分 析 ： " 第 二 次 买 套 餐 了 ， 这 次 没 有 送 的 小 瓶 装 的 护 发 素 ， 东 西 挺 好 的 " 的 情 感 是 ？ 选 项 ： 负 面 ， 正 面 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [正 面]
-----------------------------------------

-----------------------------------------
Dataset [TouzizhidaoDataset] with task id [35].
An example: [问 答 判 断 ： " 关 于 保 险 合 同 解 除 权 ， 解 除 合 同 时 的 通 知 义 务 什 么 意 思 " 的 回 答 " 关 于 保 险 合 同 解 除 权 ， 解 除 合 同 时 的 通 知 义 务 是 保 险 合 同 成 立 后 ， 当 事 人 一 方 依 据 保 险 法 主 张 解 除 合 同 的 ， 应 当 书 面 通 知 对 方 ， 保 险 合 同 自 通 知 书 送 达 对 方 时 解 除 。 协 议 解 除 的 ， 保 险 合 同 自 达 成 解 除 合 同 的 协 议 时 解 除 。 合 同 另 有 约 定 的 ， 依 约 定 。 保 险 合 同 解 除 权 ： 1 、 解 除 ， 是 指 在 保 险 合 同 关 系 有 效 期 限 尚 未 届 满 前 ， 保 险 合 同 当 事 人 依 法 提 前 终 止 保 险 合 同 效 力 的 法 律 行 为 。 保 险 合 同 解 除 ， 一 般 由 有 解 除 权 的 一 方 向 他 方 作 出 意 思 表 示 ， 使 已 经 成 立 的 保 险 合 同 归 于 消 灭 。 2 、 根 据 《 合 同 法 》 的 一 般 原 理 ， 依 法 成 立 的 保 险 合 同 关 系 ， 是 一 种 民 事 法 律 关 系 ， 对 合 同 当 事 人 均 有 法 律 约 束 力 。 当 事 人 必 须 严 格 按 照 合 同 的 约 定 履 行 自 己 承 诺 的 义 务 和 责 任 ， 任 何 一 方 不 得 擅 自 变 更 或 解 除 依 法 成 立 的 合 同 。 但 是 法 律 也 允 许 在 一 定 的 情 况 下 变 更 或 者 解 除 已 经 依 法 成 立 且 发 生 法 律 效 力 的 合 同 。 3 、 保 险 合 同 的 解 除 与 变 更 、 无 效 属 于 不 同 的 法 律 性 质 ， 不 能 混 为 一 谈 。 关 于 解 除 与 变 更 的 问 题 ， 变 更 是 维 持 合 同 的 法 律 效 力 ， 只 是 对 合 同 当 事 人 的 权 利 义 务 或 法 律 关 系 做 一 些 符 合 ， 不 符 ，]
Its label is [不 符]
-----------------------------------------

-----------------------------------------
Dataset [CMNLIDataset] with task id [2].
An example: [意 思 判 别 ： " 当 鞭 子 们 朝 混 乱 的 方 向 跑 去 时 ， 战 俘 营 里 的 人 们 大 声 喊 叫 起 来 。 " 与 " 鞭 子 们 从 呼 喊 的 声 音 中 迅 速 地 跑 开 了 。 " 的 关 系 是 ？ 选 项 ： 矛 盾 ， 中 立 ， 蕴 含 ， [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [矛 盾]
-----------------------------------------

-----------------------------------------
Dataset [Cmrc2018Dataset] with task id [12].
An example: [抽 取 式 问 答 ： 文 本 " 福 井 舞 （ ） 是 出 身 于 京 都 府 京 都 市 的 日 本 女 创 作 歌 手 ， 23 岁 时 出 道 ， 血 型 a 型 ， 所 属 唱 片 公 司 为 j - more 。 2004 年 ， 与 wadagaki 、 shino 组 合 地 下 音 乐 队 poplar ， 发 表 了 两 张 专 辑 ， 天 照 和 梦 死 物 语 。 在 2006 年 时 退 出 ， 2007 年 10 月 加 入 了 avex 独 立 发 展 ， 2008 年 2 月 下 旬 自 京 都 上 东 京 。 2010 年 8 月 20 日 ， 以 单 曲 「 爱 之 歌 」 初 次 亮 相 ， 同 年 9 月 同 曲 的 原 唱 铃 声 下 载 数 目 超 过 50 万 。 11 月 19 日 ， 作 为 第 二 张 的 单 曲 发 表 「 lucky 」 成 为 了 初 次 亮 相 的 契 机 的 。 使 用 小 时 学 钢 琴 的 琴 进 行 作 曲 ， 据 说 作 词 是 先 以 英 语 写 完 ， 之 后 转 换 成 日 语 完 成 的 ， 作 词 及 作 曲 都 是 以 「 maifukui 」 的 名 义 发 表 。 问 题 " 福 井 舞 退 出 地 下 音 乐 队 poplar 后 加 入 了 哪 里 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]]
Its label is [avex]
-----------------------------------------

train_loss 1.710341969691217 2000
train_loss 1.5799499284476042 4000
train_loss 1.520824464313686 6000
train_loss 1.495606308080256 8000
train_loss 1.4680387502089143 10000
train_loss 1.492498700156808 12000
train_loss 1.501047035217285 14000
train_loss 1.4787876083999871 16000
train_loss 1.4422905904874206 18000
train_loss 1.4358075236454606 20000
train_loss 1.4309703933671116 22000
train_loss 1.3735542375072838 24000
train_loss 1.4313101022951304 26000
train_loss 1.3272410471737386 28000
train_loss 1.360000771857798 30000
train_loss 1.3121219765618444 32000
train_loss 1.3085078926384448 34000
train_loss 1.256256125178188 36000
train_loss 1.1978594217300416 38000
train_loss 1.1967991989143192 40000
train_loss 1.128816851163283 42000
train_loss 1.1598048109523953 44000
train_loss 1.1598085522651673 46000
train_loss 1.106376639124006 48000
train_loss 1.1228476646393537 50000
Validation loss 1.034163585577836, avg acc 0.6286509037017822, task 0 acc 0.6816588640213013, task 1 acc 0.5600489974021912, task 2 acc 0.3343912959098816, task 3 acc 0.5403225421905518, task 4 acc 0.6774839758872986, task 5 acc 0.48683682084083557, task 6 acc 0.5695422291755676, task 7 acc 0.4717610776424408, task 8 acc 0.2641369104385376, task 9 acc 0.49838361967938505, task 10 acc 0.44351251708810224, task 11 acc 0.48374998569488525, task 12 acc 0.4045005390118911, task 13 acc 0.5, task 14 acc 0.8864583969116211, task 15 acc 0.4404762089252472, task 16 acc 0.5835598111152649, task 17 acc 0.8065476417541504, task 18 acc 0.3505434989929199, task 19 acc 0.7568359375, task 20 acc 0.3307291865348816, task 21 acc 0.9495443105697632, task 22 acc 0.7410723487576786, task 23 acc 0.8392611442340071, task 24 acc 0.7383395433425903, task 25 acc 0.572509765625, task 26 acc 0.6961805820465088, task 27 acc 0.465576171875, task 28 acc 0.7869151830673218, task 29 acc 0.3680827021598816, task 30 acc 0.7841796875, task 31 acc 0.7982594966888428, task 32 acc 0.780029296875, task 33 acc 0.8175504803657532, task 34 acc 0.9176682829856873, task 35 acc 0.8192545771598816, task 36 acc 0.873798131942749, task 37 acc 0.8690362572669983
train_loss 1.1002371039763092 52000
train_loss 1.103136071334593 54000
train_loss 1.0989469822291285 56000
train_loss 1.0987176767494529 58000
train_loss 1.0545131598226727 60000
train_loss 1.0307707022931427 62000
train_loss 1.0506618936080485 64000
train_loss 1.0849412737395614 66000
train_loss 1.0380455790013075 68000
train_loss 1.0303535937443375 70000
train_loss 1.0779794347696006 72000
train_loss 1.021393148623407 74000
train_loss 1.0189132011532784 76000
train_loss 1.0726251636240631 78000
train_loss 1.0458133283983917 80000
train_loss 1.055607642583549 82000
train_loss 1.0205954104270787 84000
train_loss 1.0061953643858432 86000
train_loss 1.004521264500916 88000
train_loss 0.9959063188247382 90000
train_loss 0.9841026766225696 92000
train_loss 1.0178014491666107 94000
train_loss 1.0030751073416322 96000
train_loss 1.0008738991729915 98000
train_loss 1.0515667445380241 100000
Validation loss 0.8738478300008178, avg acc 0.6741178035736084, task 0 acc 0.6819509267807007, task 1 acc 0.5557597875595093, task 2 acc 0.3688151240348816, task 3 acc 0.5, task 4 acc 0.8459535241127014, task 5 acc 0.5460208654403687, task 6 acc 0.6478872895240784, task 7 acc 0.5122884511947632, task 8 acc 0.3005952537059784, task 9 acc 0.503855280869316, task 10 acc 0.6078289067849963, task 11 acc 0.48124998807907104, task 12 acc 0.5669211088205347, task 13 acc 0.59375, task 14 acc 0.9125000238418579, task 15 acc 0.6160714626312256, task 16 acc 0.5822011232376099, task 17 acc 0.805059552192688, task 18 acc 0.43546196818351746, task 19 acc 0.80322265625, task 20 acc 0.3667806088924408, task 21 acc 0.9495443105697632, task 22 acc 0.8844604831219021, task 23 acc 0.8815355522791071, task 24 acc 0.8451492190361023, task 25 acc 0.6097819209098816, task 26 acc 0.6857638955116272, task 27 acc 0.5677897334098816, task 28 acc 0.7870979309082031, task 29 acc 0.400390625, task 30 acc 0.7861328125, task 31 acc 0.7990506291389465, task 32 acc 0.78564453125, task 33 acc 0.8282828330993652, task 34 acc 0.9216746687889099, task 35 acc 0.8193359375, task 36 acc 0.879807710647583, task 37 acc 0.9508587718009949
train_loss 1.0024627916812896 102000
train_loss 0.9770002660993486 104000
train_loss 0.9564906720090658 106000
train_loss 0.9575103300809861 108000
train_loss 0.9528044959213585 110000
train_loss 0.9290097547285259 112000
train_loss 0.9248543297424913 114000
train_loss 0.9471799672171474 116000
train_loss 0.9411971981450915 118000
train_loss 0.9507729572113603 120000
train_loss 0.9775085813514889 122000
train_loss 0.9270179082620889 124000
train_loss 0.9396908368580044 126000
train_loss 0.9383474170267582 128000
train_loss 0.90440423287265 130000
train_loss 0.9167159013357014 132000
train_loss 0.9069378363471479 134000
train_loss 0.9238226060848683 136000
train_loss 0.924662868609652 138000
train_loss 0.9184995202682912 140000
train_loss 0.8824566304162145 142000
train_loss 0.9164842941146344 144000
train_loss 0.9117488146889955 146000
train_loss 0.9372923422660678 148000
train_loss 0.8842182611469179 150000
Validation loss 0.8154610500230774, avg acc 0.6852148771286011, task 0 acc 0.6816588640213013, task 1 acc 0.5608659982681274, task 2 acc 0.3540852963924408, task 3 acc 0.5211693644523621, task 4 acc 0.8894230723381042, task 5 acc 0.5268287658691406, task 6 acc 0.6954225301742554, task 7 acc 0.5423177480697632, task 8 acc 0.2991071343421936, task 9 acc 0.42018227233189215, task 10 acc 0.6411535961299444, task 11 acc 0.5099999904632568, task 12 acc 0.6068369181547717, task 13 acc 0.6145833730697632, task 14 acc 0.9229167103767395, task 15 acc 0.6949405074119568, task 16 acc 0.585597813129425, task 17 acc 0.8065476417541504, task 18 acc 0.4789402186870575, task 19 acc 0.84130859375, task 20 acc 0.3599446713924408, task 21 acc 0.9495443105697632, task 22 acc 0.8893939471950261, task 23 acc 0.8824335559265005, task 24 acc 0.8731343150138855, task 25 acc 0.6121419668197632, task 26 acc 0.7048611044883728, task 27 acc 0.5741373896598816, task 28 acc 0.7874634265899658, task 29 acc 0.4012044370174408, task 30 acc 0.789306640625, task 31 acc 0.8006329536437988, task 32 acc 0.7869466543197632, task 33 acc 0.8317550420761108, task 34 acc 0.9312900900840759, task 35 acc 0.8173014521598816, task 36 acc 0.8870192766189575, task 37 acc 0.9657681584358215
train_loss 0.8943631262201815 152000
train_loss 0.9115666852649301 154000
train_loss 0.8885083396472037 156000
train_loss 0.890396392704919 158000
train_loss 0.8826128620393574 160000
train_loss 0.9027269488610328 162000
train_loss 0.9252443489450961 164000
train_loss 0.9092571385037154 166000
train_loss 0.8658412962798029 168000
train_loss 0.8918470365479588 170000
train_loss 0.8779997576056048 172000
train_loss 0.910540463312529 174000
train_loss 0.8877678201161325 176000
train_loss 0.9161965557467192 178000
train_loss 0.9147048327596858 180000
train_loss 0.8934068865571171 182000
train_loss 0.8836327065047808 184000
train_loss 0.9025233961939811 186000
train_loss 0.8907283185496926 188000
train_loss 0.8653227242529392 190000
train_loss 0.8865612151855603 192000
train_loss 0.8912318345280364 194000
train_loss 0.8654938135156408 196000
train_loss 0.8869218001915142 198000
train_loss 0.8838289724169299 200000
Validation loss 0.7973839221083575, avg acc 0.6837876439094543, task 0 acc 0.6816588640213013, task 1 acc 0.4832516312599182, task 2 acc 0.3473307490348816, task 3 acc 0.5090725421905518, task 4 acc 0.8946314454078674, task 5 acc 0.5078375935554504, task 6 acc 0.6998239159584045, task 7 acc 0.5686849355697632, task 8 acc 0.3035714328289032, task 9 acc 0.4498219327240231, task 10 acc 0.664338040352497, task 11 acc 0.47999998927116394, task 12 acc 0.6484927886079266, task 13 acc 0.5625, task 14 acc 0.9197916984558105, task 15 acc 0.7053571343421936, task 16 acc 0.5835598111152649, task 17 acc 0.7976190447807312, task 18 acc 0.4884510934352875, task 19 acc 0.8307291865348816, task 20 acc 0.357177734375, task 21 acc 0.9495443105697632, task 22 acc 0.9058333835865536, task 23 acc 0.8950032392345757, task 24 acc 0.8610074520111084, task 25 acc 0.6149088740348816, task 26 acc 0.671875, task 27 acc 0.5723470449447632, task 28 acc 0.787920355796814, task 29 acc 0.431884765625, task 30 acc 0.7889811396598816, task 31 acc 0.8002373576164246, task 32 acc 0.7852376699447632, task 33 acc 0.8301767706871033, task 34 acc 0.9326923489570618, task 35 acc 0.812744140625, task 36 acc 0.8882212042808533, task 37 acc 0.9716125726699829
train_loss 0.886590258573182 202000
train_loss 0.8703909785356373 204000
train_loss 0.8915346538163722 206000
train_loss 0.8727952565168962 208000
train_loss 0.9032101995963603 210000
train_loss 0.8618971065976657 212000
train_loss 0.862598568298854 214000
train_loss 0.881373132487759 216000
train_loss 0.8500518952533602 218000
train_loss 0.880741401489824 220000
train_loss 0.9025545613276772 222000
train_loss 0.8264427602393553 224000
train_loss 0.8626911964002065 226000
train_loss 0.8866579625913873 228000
train_loss 0.8539953613444231 230000
train_loss 0.8474301067516208 232000
train_loss 0.8495261431927793 234000
train_loss 0.8465572610879317 236000
train_loss 0.868756151439622 238000
train_loss 0.8754577722018585 240000
train_loss 0.8342450472107157 242000
train_loss 0.8450224819853902 244000
train_loss 0.8537370608584024 246000
train_loss 0.8367450481266715 248000
train_loss 0.867241547562182 250000
Validation loss 0.7664964772329442, avg acc 0.6951783895492554, task 0 acc 0.6816588640213013, task 1 acc 0.5600489974021912, task 2 acc 0.3458659052848816, task 3 acc 0.5075604915618896, task 4 acc 0.9060496687889099, task 5 acc 0.536374568939209, task 6 acc 0.7315140962600708, task 7 acc 0.573974609375, task 8 acc 0.2886904776096344, task 9 acc 0.45418847782361066, task 10 acc 0.6773457823471161, task 11 acc 0.5331249833106995, task 12 acc 0.6815309364959331, task 13 acc 0.5833333730697632, task 14 acc 0.9354166984558105, task 15 acc 0.6979166865348816, task 16 acc 0.5835598111152649, task 17 acc 0.8110119104385376, task 18 acc 0.539402186870575, task 19 acc 0.8456217646598816, task 20 acc 0.337158203125, task 21 acc 0.9495443105697632, task 22 acc 0.917207536655343, task 23 acc 0.8969435804330784, task 24 acc 0.8903917670249939, task 25 acc 0.611083984375, task 26 acc 0.7083333134651184, task 27 acc 0.5702311396598816, task 28 acc 0.7895650863647461, task 29 acc 0.42626953125, task 30 acc 0.790283203125, task 31 acc 0.8006329536437988, task 32 acc 0.791259765625, task 33 acc 0.8355429172515869, task 34 acc 0.9318910241127014, task 35 acc 0.8264974355697632, task 36 acc 0.8906250596046448, task 37 acc 0.9791269302368164
train_loss 0.8446417724974453 252000
train_loss 0.8468047363371588 254000
train_loss 0.8604896310800686 256000
train_loss 0.8783553783791139 258000
train_loss 0.8499691088376566 260000
train_loss 0.8736057764319703 262000
train_loss 0.8356579983923584 264000
train_loss 0.854279684945941 266000
train_loss 0.8337554549761117 268000
train_loss 0.8463927558166906 270000
train_loss 0.8405986856697127 272000
train_loss 0.8441933574876748 274000
train_loss 0.8189298187680543 276000
train_loss 0.8372140633612871 278000
train_loss 0.8775033339308574 280000
train_loss 0.8339353375770152 282000
train_loss 0.8588092874391005 284000
train_loss 0.8490615473068319 286000
train_loss 0.8369964297804982 288000
train_loss 0.8309491009349004 290000
train_loss 0.8427999388254248 292000
train_loss 0.8390219744881615 294000
train_loss 0.8331437584678643 296000
train_loss 0.8427669542231597 298000
train_loss 0.8177445598011837 300000
Validation loss 0.7395148962131103, avg acc 0.6938220262527466, task 0 acc 0.6816588640213013, task 1 acc 0.4589460790157318, task 2 acc 0.3329264521598816, task 3 acc 0.4873991906642914, task 4 acc 0.9050480723381042, task 5 acc 0.5236133337020874, task 6 acc 0.7297534942626953, task 7 acc 0.5787760615348816, task 8 acc 0.2916666865348816, task 9 acc 0.43875231350148447, task 10 acc 0.719919293707337, task 11 acc 0.5218749642372131, task 12 acc 0.683702538840045, task 13 acc 0.5833333730697632, task 14 acc 0.9322916865348816, task 15 acc 0.7366071343421936, task 16 acc 0.5849184989929199, task 17 acc 0.8110119104385376, task 18 acc 0.5808423757553101, task 19 acc 0.853759765625, task 20 acc 0.3308919370174408, task 21 acc 0.9495443105697632, task 22 acc 0.9254438996668725, task 23 acc 0.9006542844075643, task 24 acc 0.8833954930305481, task 25 acc 0.619140625, task 26 acc 0.7239583134651184, task 27 acc 0.5690104365348816, task 28 acc 0.7798793911933899, task 29 acc 0.4378255307674408, task 30 acc 0.7848307490348816, task 31 acc 0.8026107549667358, task 32 acc 0.7845866084098816, task 33 acc 0.8226010203361511, task 34 acc 0.9308894276618958, task 35 acc 0.8203939199447632, task 36 acc 0.8834134936332703, task 37 acc 0.9793654680252075
train_loss 0.8184566283589229 302000
train_loss 0.8254887027610094 304000
train_loss 0.8334168273280375 306000
train_loss 0.8183982554934919 308000
train_loss 0.8342225016024895 310000
train_loss 0.8462708269548602 312000
train_loss 0.8061308688935823 314000
train_loss 0.7990238215446006 316000
train_loss 0.8048427240638993 318000
train_loss 0.8084299557134509 320000
train_loss 0.8093188594700769 322000
train_loss 0.8012055910155177 324000
train_loss 0.8012684990409762 326000
train_loss 0.8266957166278734 328000
train_loss 0.8298948963461443 330000
train_loss 0.8275174930924549 332000
train_loss 0.8344445191167761 334000
train_loss 0.805400772377383 336000
train_loss 0.8129004313750192 338000
train_loss 0.8040177135365084 340000
train_loss 0.8039049602691084 342000
train_loss 0.8157195033216849 344000
train_loss 0.8300439215744846 346000
train_loss 0.849122978697531 348000
train_loss 0.8404736841390841 350000
Validation loss 0.7422645369225158, avg acc 0.6978805661201477, task 0 acc 0.6816588640213013, task 1 acc 0.5065359473228455, task 2 acc 0.357666015625, task 3 acc 0.5005040168762207, task 4 acc 0.9082531929016113, task 5 acc 0.5142684578895569, task 6 acc 0.6681337952613831, task 7 acc 0.587158203125, task 8 acc 0.2790178656578064, task 9 acc 0.4695150464750275, task 10 acc 0.7111884010534414, task 11 acc 0.5118749737739563, task 12 acc 0.7157398684367597, task 13 acc 0.5833333730697632, task 14 acc 0.9447917342185974, task 15 acc 0.7172619104385376, task 16 acc 0.5835598111152649, task 17 acc 0.7976190447807312, task 18 acc 0.5883152484893799, task 19 acc 0.847412109375, task 20 acc 0.364990234375, task 21 acc 0.9495443105697632, task 22 acc 0.9204069596777895, task 23 acc 0.9017537598754537, task 24 acc 0.8899253606796265, task 25 acc 0.6275228261947632, task 26 acc 0.7291666865348816, task 27 acc 0.5786947011947632, task 28 acc 0.7872806787490845, task 29 acc 0.4239909052848816, task 30 acc 0.7974447011947632, task 31 acc 0.8061708807945251, task 32 acc 0.796142578125, task 33 acc 0.839330792427063, task 34 acc 0.9362980723381042, task 35 acc 0.8304850459098816, task 36 acc 0.8870192766189575, task 37 acc 0.9794847369194031
train_loss 0.8176150772990659 352000
train_loss 0.7918056790865958 354000
train_loss 0.816586422235705 356000
train_loss 0.8164047165671363 358000
train_loss 0.8157457151645795 360000
train_loss 0.8084150144569576 362000
train_loss 0.8262656111074611 364000
train_loss 0.7948557313997299 366000
train_loss 0.7865342107198667 368000
train_loss 1.0008562316615135 370000
train_loss 0.9654785733576864 372000
train_loss 0.9065663619479164 374000
train_loss 0.8219828438088298 376000
train_loss 0.8242478033518419 378000
train_loss 0.7958958668406122 380000
train_loss 0.8233902794574387 382000
train_loss 0.790175984068308 384000
train_loss 0.8054498059377074 386000
train_loss 0.8149249965467025 388000
train_loss 0.8185412338362076 390000
train_loss 0.7785897875349037 392000
train_loss 0.7996172108510509 394000
train_loss 0.8132359871962108 396000
train_loss 0.8028982909265906 398000
train_loss 0.7907373510822654 400000
Validation loss 0.716550715242067, avg acc 0.7054024934768677, task 0 acc 0.6816588640213013, task 1 acc 0.45649510622024536, task 2 acc 0.3658854365348816, task 3 acc 0.5075604915618896, task 4 acc 0.9056490659713745, task 5 acc 0.5077371001243591, task 6 acc 0.7522006630897522, task 7 acc 0.5904134511947632, task 8 acc 0.3303571343421936, task 9 acc 0.5002063447946599, task 10 acc 0.7296409142224185, task 11 acc 0.4793749749660492, task 12 acc 0.7238706907791995, task 13 acc 0.5833333730697632, task 14 acc 0.9447917342185974, task 15 acc 0.7901785969734192, task 16 acc 0.5835598111152649, task 17 acc 0.8080357313156128, task 18 acc 0.6066576242446899, task 19 acc 0.8534342646598816, task 20 acc 0.3673502802848816, task 21 acc 0.9495443105697632, task 22 acc 0.9224363280907608, task 23 acc 0.8995650590166654, task 24 acc 0.8959887623786926, task 25 acc 0.6311849355697632, task 26 acc 0.7291666865348816, task 27 acc 0.5801595449447632, task 28 acc 0.7944992780685425, task 29 acc 0.4546712338924408, task 30 acc 0.801025390625, task 31 acc 0.8101266026496887, task 32 acc 0.80078125, task 33 acc 0.8434343338012695, task 34 acc 0.9328926205635071, task 35 acc 0.830810546875, task 36 acc 0.8810096383094788, task 37 acc 0.9796040058135986
train_loss 0.808420784376096 402000
train_loss 0.7909739888417534 404000
train_loss 0.7949958217181265 406000
train_loss 0.7942064563306048 408000
train_loss 0.7993237185049802 410000
train_loss 0.7673869884060696 412000
train_loss 0.8066090885372832 414000
train_loss 0.7979963928167708 416000
train_loss 0.799840341874864 418000
train_loss 0.7847419428983703 420000
train_loss 0.7930092560001649 422000
train_loss 0.7855318408096209 424000
train_loss 0.78067066198471 426000
train_loss 0.7761883414140902 428000
train_loss 0.762039591328241 430000
train_loss 0.7716113249626942 432000
train_loss 0.7785525560779497 434000
train_loss 0.7829508326349315 436000
train_loss 0.7892248335210607 438000
train_loss 0.8039006839971989 440000
train_loss 0.8006732574915514 442000
train_loss 0.7911198388305493 444000
train_loss 0.8715257561756298 446000
train_loss 0.8289053504858166 448000
train_loss 0.795044376321137 450000
Validation loss 0.7249238623745454, avg acc 0.710171639919281, task 0 acc 0.6816588640213013, task 1 acc 0.5404411554336548, task 2 acc 0.3740234375, task 3 acc 0.49697577953338623, task 4 acc 0.9102564454078674, task 5 acc 0.4936695992946625, task 6 acc 0.7491196990013123, task 7 acc 0.5972493886947632, task 8 acc 0.3504464328289032, task 9 acc 0.5658285675178112, task 10 acc 0.7354217851045713, task 11 acc 0.5212500095367432, task 12 acc 0.7408979879807265, task 13 acc 0.5833333730697632, task 14 acc 0.9406250715255737, task 15 acc 0.7485119104385376, task 16 acc 0.5835598111152649, task 17 acc 0.816964328289032, task 18 acc 0.5740489363670349, task 19 acc 0.8525390625, task 20 acc 0.3792317807674408, task 21 acc 0.9495443105697632, task 22 acc 0.9320415358929545, task 23 acc 0.9059242466521316, task 24 acc 0.8969216346740723, task 25 acc 0.623046875, task 26 acc 0.7361111044883728, task 27 acc 0.58154296875, task 28 acc 0.7961440086364746, task 29 acc 0.4568684995174408, task 30 acc 0.8057454824447632, task 31 acc 0.7994462251663208, task 32 acc 0.8006998896598816, task 33 acc 0.8428030014038086, task 34 acc 0.9356971383094788, task 35 acc 0.8260905146598816, task 36 acc 0.8858173489570618, task 37 acc 0.9760257601737976
train_loss 0.7977773656090722 452000
train_loss 0.7750716671347618 454000
train_loss 0.7752270931387321 456000
train_loss 0.7879174099368974 458000
train_loss 0.7820467463606037 460000
train_loss 0.7855489809755236 462000
train_loss 0.775432991521433 464000
train_loss 0.7869481156677939 466000
train_loss 0.801776142856339 468000
train_loss 0.7990463904109784 470000
train_loss 0.8240014182855375 472000
train_loss 0.8182408126760274 474000
train_loss 0.783665462821722 476000
train_loss 0.785279487228021 478000
train_loss 0.8124429745785892 480000
train_loss 0.7970425402736291 482000
train_loss 0.8312876793579198 484000
train_loss 0.7905803539343178 486000
train_loss 0.8090141044193879 488000
train_loss 0.7708245466072112 490000
train_loss 0.7878333086678758 492000
train_loss 0.8040474901003763 494000
train_loss 0.7921867414079606 496000
train_loss 0.763609730207827 498000
train_loss 0.7688200323614292 500000
Validation loss 0.7091493840355506, avg acc 0.7122350931167603, task 0 acc 0.6816588640213013, task 1 acc 0.5594362616539001, task 2 acc 0.3743489682674408, task 3 acc 0.49546369910240173, task 4 acc 0.9116586446762085, task 5 acc 0.5101487040519714, task 6 acc 0.7557218074798584, task 7 acc 0.6045736074447632, task 8 acc 0.3258928656578064, task 9 acc 0.5573029520405006, task 10 acc 0.7383498614555457, task 11 acc 0.48249998688697815, task 12 acc 0.7121923115155794, task 13 acc 0.5833333730697632, task 14 acc 0.9427083730697632, task 15 acc 0.7976190447807312, task 16 acc 0.5835598111152649, task 17 acc 0.8154761791229248, task 18 acc 0.6474184989929199, task 19 acc 0.8566080927848816, task 20 acc 0.38134765625, task 21 acc 0.9495443105697632, task 22 acc 0.9257554832124831, task 23 acc 0.9053234316643634, task 24 acc 0.9057835340499878, task 25 acc 0.6241048574447632, task 26 acc 0.7517361044883728, task 27 acc 0.5732421875, task 28 acc 0.7993420958518982, task 29 acc 0.4540202021598816, task 30 acc 0.8035482168197632, task 31 acc 0.8144778609275818, task 32 acc 0.797607421875, task 33 acc 0.8285984992980957, task 34 acc 0.9348958730697632, task 35 acc 0.8213704824447632, task 36 acc 0.8858173489570618, task 37 acc 0.9724475145339966
train_loss 0.7980332918157801 502000
train_loss 0.7644730554488488 504000
train_loss 0.8051452115247958 506000
train_loss 0.7689075577845796 508000
train_loss 0.8011130161485635 510000
train_loss 0.7893822451480664 512000
train_loss 0.7816600149571895 514000
train_loss 0.7728533780472353 516000
train_loss 0.7894447727738879 518000
train_loss 0.7701371101681143 520000
train_loss 0.78840861049667 522000
train_loss 0.7980515611916781 524000
train_loss 0.7945694778379985 526000
train_loss 0.7703396611334756 528000
train_loss 0.7671306310766377 530000
train_loss 0.7932665220135823 532000
train_loss 0.7969928103322164 534000
train_loss 0.77598614457855 536000
train_loss 0.8038990488108247 538000
train_loss 0.7637215469870716 540000
train_loss 0.7678018460310996 542000
train_loss 0.800472570468206 544000
train_loss 0.7699571762853302 546000
train_loss 0.7867396638309583 548000
train_loss 0.7788719450086355 550000
Validation loss 0.7058868627510961, avg acc 0.7091584205627441, task 0 acc 0.6816588640213013, task 1 acc 0.5549428462982178, task 2 acc 0.3667806088924408, task 3 acc 0.5115927457809448, task 4 acc 0.9052484035491943, task 5 acc 0.5061293840408325, task 6 acc 0.7513204216957092, task 7 acc 0.60400390625, task 8 acc 0.3563988208770752, task 9 acc 0.4976843585902424, task 10 acc 0.7207358435970478, task 11 acc 0.47874999046325684, task 12 acc 0.7373658579583928, task 13 acc 0.53125, task 14 acc 0.9489583969116211, task 15 acc 0.7678571343421936, task 16 acc 0.5788043737411499, task 17 acc 0.8110119104385376, task 18 acc 0.63451087474823, task 19 acc 0.8550618886947632, task 20 acc 0.35400390625, task 21 acc 0.9495443105697632, task 22 acc 0.9196855397761178, task 23 acc 0.8987393457237505, task 24 acc 0.8978544473648071, task 25 acc 0.6249186396598816, task 26 acc 0.7708333134651184, task 27 acc 0.5867513418197632, task 28 acc 0.7979714870452881, task 29 acc 0.4524739682674408, task 30 acc 0.8085123896598816, task 31 acc 0.824367105960846, task 32 acc 0.7939453125, task 33 acc 0.8465909361839294, task 34 acc 0.9379006624221802, task 35 acc 0.8216959834098816, task 36 acc 0.8810096383094788, task 37 acc 0.9811545610427856
train_loss 0.7958437452483922 552000
train_loss 0.7813509629634209 554000
train_loss 0.7838902527368627 556000
train_loss 0.7840128692174331 558000
train_loss 0.7909319650093094 560000
train_loss 0.7830794628541916 562000
train_loss 0.7835119036827236 564000
train_loss 0.7940797686204314 566000
train_loss 0.8101791497124359 568000
train_loss 0.7726722809299826 570000
train_loss 0.7979756576418877 572000
train_loss 0.7932403490087018 574000
train_loss 0.8013612247761339 576000
train_loss 0.7762081027636305 578000
train_loss 0.7665304988562129 580000
train_loss 0.7842314490270801 582000
train_loss 0.785170969019644 584000
train_loss 0.7813298004767858 586000
train_loss 0.7804757155883126 588000
train_loss 0.7586121437167749 590000
train_loss 0.7960883633485064 592000
train_loss 0.784547524894122 594000
train_loss 0.7879958914779127 596000
train_loss 0.7671935244905762 598000
train_loss 0.7664669957952573 600000
Validation loss 0.7173510392567319, avg acc 0.7110460996627808, task 0 acc 0.6816588640213013, task 1 acc 0.5598447918891907, task 2 acc 0.3817545771598816, task 3 acc 0.5015121102333069, task 4 acc 0.9034455418586731, task 5 acc 0.5387861728668213, task 6 acc 0.7178696990013123, task 7 acc 0.59716796875, task 8 acc 0.324404776096344, task 9 acc 0.5802212953641444, task 10 acc 0.7058343322032109, task 11 acc 0.5012499690055847, task 12 acc 0.7246285694524657, task 13 acc 0.5833333730697632, task 14 acc 0.9500000476837158, task 15 acc 0.7529761791229248, task 16 acc 0.5801630616188049, task 17 acc 0.8110119104385376, task 18 acc 0.5652173757553101, task 19 acc 0.859130859375, task 20 acc 0.3898112177848816, task 21 acc 0.9495443105697632, task 22 acc 0.9250254194012345, task 23 acc 0.8987861015502229, task 24 acc 0.8908581733703613, task 25 acc 0.6295573115348816, task 26 acc 0.7534722089767456, task 27 acc 0.5828450918197632, task 28 acc 0.7994334697723389, task 29 acc 0.4527994990348816, task 30 acc 0.8102213740348816, task 31 acc 0.8374208807945251, task 32 acc 0.8058268427848816, task 33 acc 0.8472222089767456, task 34 acc 0.9350961446762085, task 35 acc 0.8263346552848816, task 36 acc 0.884615421295166, task 37 acc 0.9806774854660034
train_loss 0.7681927879326976 602000
train_loss 0.7785328949056566 604000
train_loss 0.7862151406919584 606000
train_loss 0.7681108516026288 608000
train_loss 0.7769530718103052 610000
train_loss 0.7681658824803308 612000
train_loss 0.7937367935786024 614000
train_loss 0.790228689213749 616000
train_loss 0.7859096092367545 618000
train_loss 0.8140695975907147 620000
train_loss 0.7856583279157058 622000
train_loss 0.7913381972471252 624000
train_loss 0.8181926398482173 626000
train_loss 0.7678815839882008 628000
train_loss 0.7779699462247082 630000
train_loss 0.7896199400927871 632000
train_loss 0.7818588846619241 634000
train_loss 0.7995795968803577 636000
train_loss 0.7980914552891627 638000
train_loss 0.7905324806617573 640000
train_loss 0.7663912901747971 642000
train_loss 0.7877891422389075 644000
train_loss 0.7934496039790101 646000
train_loss 0.7911589742901269 648000
train_loss 0.7884132829969749 650000
Validation loss 0.7081215390653108, avg acc 0.7174690961837769, task 0 acc 0.6816588640213013, task 1 acc 0.5600489974021912, task 2 acc 0.4348958432674408, task 3 acc 0.4889112710952759, task 4 acc 0.9098557829856873, task 5 acc 0.5375803709030151, task 6 acc 0.7266725301742554, task 7 acc 0.5996907949447632, task 8 acc 0.3861607313156128, task 9 acc 0.43884023482951573, task 10 acc 0.7261975942647941, task 11 acc 0.5674999952316284, task 12 acc 0.7304083110304437, task 13 acc 0.5833333730697632, task 14 acc 0.9510416984558105, task 15 acc 0.773809552192688, task 16 acc 0.5835598111152649, task 17 acc 0.805059552192688, task 18 acc 0.65013587474823, task 19 acc 0.8602702021598816, task 20 acc 0.4410807490348816, task 21 acc 0.9495443105697632, task 22 acc 0.9223446339560016, task 23 acc 0.904190885533945, task 24 acc 0.902518630027771, task 25 acc 0.6336263418197632, task 26 acc 0.75, task 27 acc 0.5872396230697632, task 28 acc 0.8042763471603394, task 29 acc 0.4562174677848816, task 30 acc 0.80908203125, task 31 acc 0.8382120728492737, task 32 acc 0.8037109375, task 33 acc 0.8415403962135315, task 34 acc 0.9358974695205688, task 35 acc 0.825927734375, task 36 acc 0.8834134936332703, task 37 acc 0.9793654680252075
train_loss 0.7771566162835807 652000
train_loss 0.7814070440949872 654000
train_loss 0.7867450882010162 656000
train_loss 0.7751129300678149 658000
train_loss 0.7968358830912039 660000
train_loss 0.7841151353116147 662000
train_loss 0.7714335097623989 664000
train_loss 0.7835578302005306 666000
train_loss 0.7875142781911418 668000
train_loss 0.7875983413145877 670000
train_loss 0.7937006648378446 672000
train_loss 0.7951002344260923 674000
train_loss 0.7761923470795155 676000
train_loss 0.7898057184256613 678000
train_loss 0.7934336079645437 680000
train_loss 0.8058687851352151 682000
train_loss 0.7916329574771226 684000
train_loss 0.7917653762397822 686000
train_loss 0.7646892832731829 688000
train_loss 0.775484139339067 690000
train_loss 0.794379182041157 692000
train_loss 0.7678275967158843 694000
train_loss 0.790995095117949 696000
train_loss 0.7907962379567325 698000
train_loss 0.7777736938251183 700000
Validation loss 0.7019343263438025, avg acc 0.7200919985771179, task 0 acc 0.6816588640213013, task 1 acc 0.5604575276374817, task 2 acc 0.464111328125, task 3 acc 0.4989919066429138, task 4 acc 0.9108573794364929, task 5 acc 0.5089429020881653, task 6 acc 0.7301936745643616, task 7 acc 0.599609375, task 8 acc 0.3742559552192688, task 9 acc 0.5320616971442076, task 10 acc 0.7534964053132714, task 11 acc 0.49437499046325684, task 12 acc 0.734516946773265, task 13 acc 0.59375, task 14 acc 0.9375000596046448, task 15 acc 0.7872024178504944, task 16 acc 0.5896739363670349, task 17 acc 0.816964328289032, task 18 acc 0.6535326242446899, task 19 acc 0.8548991084098816, task 20 acc 0.4623209834098816, task 21 acc 0.9495443105697632, task 22 acc 0.9300566505550018, task 23 acc 0.9037446497717029, task 24 acc 0.8987873196601868, task 25 acc 0.6221517324447632, task 26 acc 0.7517361044883728, task 27 acc 0.581298828125, task 28 acc 0.8018091917037964, task 29 acc 0.4557291865348816, task 30 acc 0.8102213740348816, task 31 acc 0.8366297483444214, task 32 acc 0.8041178584098816, task 33 acc 0.8459596037864685, task 34 acc 0.9318910241127014, task 35 acc 0.8267415761947632, task 36 acc 0.8930288553237915, task 37 acc 0.9806774854660034
train_loss 0.7718960910267196 702000
train_loss 0.7833262910419144 704000
train_loss 0.7677198039148935 706000
train_loss 0.783600996077992 708000
train_loss 0.7915211009890772 710000
train_loss 0.7787171394275502 712000
train_loss 0.7693180154878646 714000
train_loss 0.7803425338123925 716000
train_loss 0.788789955124259 718000
train_loss 0.776414264237741 720000
train_loss 0.7897236630911939 722000
train_loss 0.7867458867365494 724000
train_loss 0.7839292146968655 726000
train_loss 0.7695391961815767 728000
train_loss 0.7868439262416214 730000
train_loss 0.7811645441851579 732000
train_loss 0.7922225341158919 734000
train_loss 0.7899358907043934 736000
train_loss 0.7921524884779938 738000
train_loss 0.7843797459704801 740000
train_loss 0.8035838852128945 742000
train_loss 0.7793306058603339 744000
train_loss 0.7818308336073533 746000
train_loss 0.8425931581985205 748000
train_loss 0.7998997034302447 750000
Validation loss 0.6989068178425694, avg acc 0.7227184772491455, task 0 acc 0.6816588640213013, task 1 acc 0.5357434749603271, task 2 acc 0.486328125, task 3 acc 0.49042338132858276, task 4 acc 0.9130609035491943, task 5 acc 0.5556672215461731, task 6 acc 0.7610034942626953, task 7 acc 0.60009765625, task 8 acc 0.386904776096344, task 9 acc 0.5171573643286119, task 10 acc 0.7460557279458806, task 11 acc 0.5737499594688416, task 12 acc 0.7354969293079016, task 13 acc 0.5833333730697632, task 14 acc 0.9531250596046448, task 15 acc 0.7723214626312256, task 16 acc 0.5822011232376099, task 17 acc 0.8020833730697632, task 18 acc 0.6385869979858398, task 19 acc 0.85791015625, task 20 acc 0.4812825620174408, task 21 acc 0.9495443105697632, task 22 acc 0.9235831842649189, task 23 acc 0.904569267144945, task 24 acc 0.9011194109916687, task 25 acc 0.6253255605697632, task 26 acc 0.7430555820465088, task 27 acc 0.5855306386947632, task 28 acc 0.8045504689216614, task 29 acc 0.4476725459098816, task 30 acc 0.8085123896598816, task 31 acc 0.8354430794715881, task 32 acc 0.8060709834098816, task 33 acc 0.8383838534355164, task 34 acc 0.9368990659713745, task 35 acc 0.8247884511947632, task 36 acc 0.8930288553237915, task 37 acc 0.9810352921485901
train_loss 0.7807678975490853 752000
train_loss 0.7842513958225027 754000
train_loss 0.7890297883800231 756000
train_loss 0.7941249250704423 758000
train_loss 0.7593823382109404 760000
train_loss 0.7914801644426771 762000
train_loss 0.7863715710183605 764000
train_loss 0.7754693541005254 766000
train_loss 0.7744329808494076 768000
train_loss 0.7808916750503704 770000
train_loss 0.8040335362427868 772000
train_loss 0.7981799575029872 774000
train_loss 0.7852626913795248 776000
train_loss 0.7742463475060649 778000
train_loss 0.7885053973356262 780000
train_loss 0.8068712746486999 782000
train_loss 0.8150211128666996 784000
train_loss 0.805502261674963 786000
train_loss 0.7795645369761623 788000
train_loss 0.7903489129465306 790000
train_loss 0.8156570602850989 792000
train_loss 0.8048304809285328 794000
train_loss 0.8065629951674491 796000
train_loss 0.8540833720173687 798000
train_loss 0.7977299432656728 800000
Validation loss 0.7109399960907857, avg acc 0.718799889087677, task 0 acc 0.6819509267807007, task 1 acc 0.5298202633857727, task 2 acc 0.48388671875, task 3 acc 0.49344757199287415, task 4 acc 0.9138621687889099, task 5 acc 0.5618970990180969, task 6 acc 0.7473591566085815, task 7 acc 0.5868327021598816, task 8 acc 0.3489583432674408, task 9 acc 0.5497079383329335, task 10 acc 0.7269516970697062, task 11 acc 0.5806249976158142, task 12 acc 0.7476946170336184, task 13 acc 0.5833333730697632, task 14 acc 0.9541667103767395, task 15 acc 0.7723214626312256, task 16 acc 0.579483687877655, task 17 acc 0.8095238208770752, task 18 acc 0.6086956858634949, task 19 acc 0.856689453125, task 20 acc 0.474609375, task 21 acc 0.9495443105697632, task 22 acc 0.9229686368584051, task 23 acc 0.9026007229742148, task 24 acc 0.8950559496879578, task 25 acc 0.6206868886947632, task 26 acc 0.7170138955116272, task 27 acc 0.585693359375, task 28 acc 0.8062865734100342, task 29 acc 0.429443359375, task 30 acc 0.806884765625, task 31 acc 0.824367105960846, task 32 acc 0.803466796875, task 33 acc 0.8314393758773804, task 34 acc 0.9366987347602844, task 35 acc 0.8233236074447632, task 36 acc 0.8870192766189575, task 37 acc 0.9800810813903809
train_loss 0.7982264426434412 802000
train_loss 0.8140488459169865 804000
train_loss 0.7959077170379459 806000
train_loss 0.788372693519108 808000
train_loss 0.7870224140798673 810000
train_loss 0.7780210114358924 812000
train_loss 0.7786418477836996 814000
train_loss 0.7920806457423605 816000
train_loss 0.7922826727549546 818000
train_loss 0.7858095259936526 820000
train_loss 0.795665728112217 822000
train_loss 0.7668456403533928 824000
train_loss 0.7685750300325453 826000
train_loss 0.7756151341053191 828000
train_loss 0.7796177404429764 830000
train_loss 0.7784788946495391 832000
train_loss 0.7630089211817831 834000
train_loss 0.7656717105589341 836000
train_loss 0.7699688746854663 838000
train_loss 0.7821348282117396 840000
train_loss 0.7630320660667494 842000
train_loss 0.7583344768709503 844000
train_loss 0.7569672557581216 846000
train_loss 0.7520593838421628 848000
train_loss 0.7679523729910143 850000
Validation loss 0.6872406275678606, avg acc 0.7301704287528992, task 0 acc 0.6816588640213013, task 1 acc 0.5414624214172363, task 2 acc 0.51904296875, task 3 acc 0.4899193346500397, task 4 acc 0.9084535241127014, task 5 acc 0.5885249376296997, task 6 acc 0.7623239159584045, task 7 acc 0.5983073115348816, task 8 acc 0.3288690447807312, task 9 acc 0.551093491290902, task 10 acc 0.7303282722936436, task 11 acc 0.6368749737739563, task 12 acc 0.7447091549357606, task 13 acc 0.5833333730697632, task 14 acc 0.9552083611488342, task 15 acc 0.7901785969734192, task 16 acc 0.6847826242446899, task 17 acc 0.8095238208770752, task 18 acc 0.6582880616188049, task 19 acc 0.8563639521598816, task 20 acc 0.5120443105697632, task 21 acc 0.9495443105697632, task 22 acc 0.9171604561823595, task 23 acc 0.9065118236709385, task 24 acc 0.9011194109916687, task 25 acc 0.6349284052848816, task 26 acc 0.7604166865348816, task 27 acc 0.5773112177848816, task 28 acc 0.8066520690917969, task 29 acc 0.4463704526424408, task 30 acc 0.809326171875, task 31 acc 0.8326740860939026, task 32 acc 0.8058268427848816, task 33 acc 0.8428030014038086, task 34 acc 0.9324920177459717, task 35 acc 0.826171875, task 36 acc 0.8834134936332703, task 37 acc 0.9824665784835815
train_loss 0.765846832391806 852000
train_loss 0.7691411781478673 854000
train_loss 0.769028588837944 856000
train_loss 0.739087281899061 858000
train_loss 0.7682819157326594 860000
train_loss 0.7666274123718031 862000
train_loss 0.7634273341464577 864000
train_loss 0.7398798580700532 866000
train_loss 0.7519843189036474 868000
train_loss 0.7631921215993352 870000
train_loss 0.7716095837941394 872000
train_loss 0.7486203887737356 874000
train_loss 0.7474451365233399 876000
train_loss 0.7631310084862635 878000
train_loss 0.7353454442359507 880000
train_loss 0.7559781491290778 882000
train_loss 0.7409215035084635 884000
train_loss 0.7571674713189713 886000
train_loss 0.7544466009095777 888000
train_loss 0.7513656307198108 890000
train_loss 0.7570380369336345 892000
train_loss 0.735995482735103 894000
train_loss 0.7510636429204606 896000
train_loss 0.7456329443335998 898000
train_loss 0.7580409770817496 900000
Validation loss 0.6653075700865406, avg acc 0.7419862747192383, task 0 acc 0.6810747385025024, task 1 acc 0.560661792755127, task 2 acc 0.5498861074447632, task 3 acc 0.5040322542190552, task 4 acc 0.9140625, task 5 acc 0.7184485197067261, task 6 acc 0.7632042169570923, task 7 acc 0.603271484375, task 8 acc 0.3816964328289032, task 9 acc 0.5560911391960563, task 10 acc 0.7529760694600359, task 11 acc 0.721875011920929, task 12 acc 0.7413521171473828, task 13 acc 0.5729166865348816, task 14 acc 0.9604167342185974, task 15 acc 0.8020833730697632, task 16 acc 0.703125, task 17 acc 0.816964328289032, task 18 acc 0.65625, task 19 acc 0.855224609375, task 20 acc 0.5418294668197632, task 21 acc 0.9495443105697632, task 22 acc 0.9248581075319574, task 23 acc 0.9049469897039885, task 24 acc 0.8978544473648071, task 25 acc 0.6206868886947632, task 26 acc 0.7430555820465088, task 27 acc 0.588134765625, task 28 acc 0.808570921421051, task 29 acc 0.460205078125, task 30 acc 0.812255859375, task 31 acc 0.844936728477478, task 32 acc 0.80859375, task 33 acc 0.8434343338012695, task 34 acc 0.9360977411270142, task 35 acc 0.8253580927848816, task 36 acc 0.889423131942749, task 37 acc 0.9800810813903809
train_loss 0.7356335905902087 902000
train_loss 0.7456786694028414 904000
train_loss 0.7480284028027673 906000
train_loss 0.7743106461018324 908000
train_loss 0.7862317689880729 910000
train_loss 0.7879067344595678 912000
train_loss 0.7509975708560087 914000
train_loss 0.7657544415513985 916000
train_loss 0.7676464473279193 918000
train_loss 0.7757388824028895 920000
train_loss 0.750004250257276 922000
train_loss 0.7564998651109636 924000
train_loss 0.7566196348336526 926000
train_loss 0.7495326594596263 928000
train_loss 0.7518502656985074 930000
train_loss 0.755215116368141 932000
train_loss 0.8084624980872032 934000
train_loss 0.7532315297657624 936000
train_loss 0.7565898781968281 938000
train_loss 0.7433099710391834 940000
train_loss 0.7495339400046505 942000
train_loss 0.7602739046763163 944000
train_loss 0.7598201558426954 946000
train_loss 0.7630860646432266 948000
train_loss 0.750468178722309 950000
Validation loss 0.6865389216524256, avg acc 0.7373932600021362, task 0 acc 0.6801985502243042, task 1 acc 0.5604575276374817, task 2 acc 0.548583984375, task 3 acc 0.5005040168762207, task 4 acc 0.9176682829856873, task 5 acc 0.7076969146728516, task 6 acc 0.7213908433914185, task 7 acc 0.6024577021598816, task 8 acc 0.4055059552192688, task 9 acc 0.5057649377591461, task 10 acc 0.740357994221742, task 11 acc 0.71875, task 12 acc 0.7477379823825513, task 13 acc 0.5729166865348816, task 14 acc 0.9531250596046448, task 15 acc 0.7782738208770752, task 16 acc 0.64673912525177, task 17 acc 0.8095238208770752, task 18 acc 0.6474184989929199, task 19 acc 0.8592122793197632, task 20 acc 0.5436198115348816, task 21 acc 0.9495443105697632, task 22 acc 0.9220770895927008, task 23 acc 0.905328877026178, task 24 acc 0.9104477167129517, task 25 acc 0.6360677480697632, task 26 acc 0.7447916865348816, task 27 acc 0.58642578125, task 28 acc 0.8055555820465088, task 29 acc 0.4671224057674408, task 30 acc 0.8115234375, task 31 acc 0.8366297483444214, task 32 acc 0.8067220449447632, task 33 acc 0.8434343338012695, task 34 acc 0.9391025900840759, task 35 acc 0.8245443105697632, task 36 acc 0.8822115659713745, task 37 acc 0.9815124273300171
train_loss 0.7682817296842113 952000
train_loss 0.772907810613513 954000
train_loss 0.7488518884284422 956000
train_loss 0.7750273193153553 958000
train_loss 0.7669867246956564 960000
train_loss 0.7572994370656088 962000
train_loss 0.7448066099351272 964000
train_loss 0.7555771866873838 966000
train_loss 0.773329558453057 968000
train_loss 0.751747829046566 970000
train_loss 0.7697379417517223 972000
train_loss 0.7525277908784337 974000
train_loss 0.761922205850482 976000
train_loss 0.778040228002239 978000
train_loss 0.7519926447381731 980000
train_loss 0.7494527629036456 982000
train_loss 0.7461354209131096 984000
train_loss 0.7364393736523344 986000
train_loss 0.7551485858084634 988000
train_loss 0.8233085375754162 990000
train_loss 0.7704153731702827 992000
train_loss 0.7580122557156719 994000
train_loss 0.7765638733534143 996000
train_loss 0.7621204258704093 998000
train_loss 0.7422029928902629 1000000
Validation loss 0.6822873017767285, avg acc 0.7422617077827454, task 0 acc 0.6758177280426025, task 1 acc 0.5441176295280457, task 2 acc 0.5494791865348816, task 3 acc 0.5095766186714172, task 4 acc 0.9160656929016113, task 5 acc 0.7122186422348022, task 6 acc 0.76408451795578, task 7 acc 0.5966796875, task 8 acc 0.4441964328289032, task 9 acc 0.5379845726438783, task 10 acc 0.7437158460640034, task 11 acc 0.7162500023841858, task 12 acc 0.7566235252923023, task 13 acc 0.5833333730697632, task 14 acc 0.9541667103767395, task 15 acc 0.7872024178504944, task 16 acc 0.71263587474823, task 17 acc 0.8199405074119568, task 18 acc 0.6460598111152649, task 19 acc 0.8565267324447632, task 20 acc 0.5423991084098816, task 21 acc 0.9495443105697632, task 22 acc 0.9221084801344217, task 23 acc 0.900586531916445, task 24 acc 0.9015858173370361, task 25 acc 0.6316732168197632, task 26 acc 0.75, task 27 acc 0.5877279043197632, task 28 acc 0.8055555820465088, task 29 acc 0.4586588740348816, task 30 acc 0.8114420771598816, task 31 acc 0.8393987417221069, task 32 acc 0.8064779043197632, task 33 acc 0.8434343338012695, task 34 acc 0.9340945482254028, task 35 acc 0.825439453125, task 36 acc 0.8882212042808533, task 37 acc 0.9809160232543945
Training recovered from step 1000000, performance at this step is 0.7422617077827454
train_loss 0.7340267452080734 1002000
train_loss 0.7462551816725173 1004000
train_loss 0.7472375263553113 1006000
train_loss 0.7675976261519827 1008000
train_loss 0.7501276141600683 1010000
train_loss 0.7577215648335405 1012000
train_loss 0.7397633297007996 1014000
train_loss 0.7484695956008509 1016000
train_loss 0.7489765702039003 1018000
train_loss 0.7525902917901985 1020000
train_loss 0.7490763475275598 1022000
train_loss 0.7366150016142056 1024000
train_loss 0.7637627095317002 1026000
train_loss 0.7297798223812133 1028000
train_loss 0.7504138095206581 1030000
train_loss 0.7396667871810496 1032000
train_loss 0.7557856507855467 1034000
train_loss 0.7567796780562493 1036000
train_loss 0.7396892241262831 1038000
train_loss 0.7421602041688747 1040000
train_loss 0.7291436877958477 1042000
train_loss 0.7569683626359328 1044000
train_loss 0.7619439507042989 1046000
train_loss 0.7297313338634558 1048000
train_loss 0.7453096212791279 1050000
Validation loss 0.6661897264276558, avg acc 0.7471250295639038, task 0 acc 0.6813668012619019, task 1 acc 0.5584150552749634, task 2 acc 0.5760905146598816, task 3 acc 0.49395158886909485, task 4 acc 0.9148637652397156, task 5 acc 0.7028737664222717, task 6 acc 0.7420774698257446, task 7 acc 0.6085612177848816, task 8 acc 0.474702388048172, task 9 acc 0.5360218888436695, task 10 acc 0.7464117604138153, task 11 acc 0.7637499570846558, task 12 acc 0.7351714900513782, task 13 acc 0.5833333730697632, task 14 acc 0.9552083611488342, task 15 acc 0.8125, task 16 acc 0.7432065606117249, task 17 acc 0.8139880895614624, task 18 acc 0.6739130616188049, task 19 acc 0.8564453125, task 20 acc 0.5735677480697632, task 21 acc 0.9495443105697632, task 22 acc 0.9223226317098236, task 23 acc 0.9069628025454557, task 24 acc 0.9034514427185059, task 25 acc 0.627197265625, task 26 acc 0.75, task 27 acc 0.5926920771598816, task 28 acc 0.8075658082962036, task 29 acc 0.4554036557674408, task 30 acc 0.8116862177848816, task 31 acc 0.8390032052993774, task 32 acc 0.807373046875, task 33 acc 0.8446969985961914, task 34 acc 0.9379006624221802, task 35 acc 0.827880859375, task 36 acc 0.8858173489570618, task 37 acc 0.9748330116271973
train_loss 0.7386339499978348 1052000
train_loss 0.7503618573993445 1054000
train_loss 0.7323071366865188 1056000
train_loss 0.7522734870533458 1058000
train_loss 0.7463673394583166 1060000
train_loss 0.7401377315660939 1062000
train_loss 0.7378061850317754 1064000
train_loss 0.758288672896102 1066000
train_loss 0.7320992748551071 1068000
train_loss 0.7344652008838021 1070000
train_loss 0.7477831074260175 1072000
train_loss 0.7306850240170024 1074000
train_loss 0.7292205170853995 1076000
train_loss 0.760770980231464 1078000
train_loss 0.746717963328585 1080000
train_loss 0.746783571338281 1082000
train_loss 0.7341539070997387 1084000
train_loss 0.7266621417398564 1086000
train_loss 0.7142069902000949 1088000
train_loss 0.7396096780155785 1090000
train_loss 0.7342138559250161 1092000
train_loss 0.7533673837799579 1094000
train_loss 0.8068658232633025 1096000
train_loss 0.7550582485860213 1098000
train_loss 0.7652096613366157 1100000
Validation loss 0.6764521303437279, avg acc 0.745932936668396, task 0 acc 0.683411180973053, task 1 acc 0.5049019455909729, task 2 acc 0.589111328125, task 3 acc 0.4899193346500397, task 4 acc 0.908052921295166, task 5 acc 0.7260851860046387, task 6 acc 0.7169893980026245, task 7 acc 0.6068522334098816, task 8 acc 0.4456845223903656, task 9 acc 0.527781098524533, task 10 acc 0.7410487499045246, task 11 acc 0.762499988079071, task 12 acc 0.7604672494372604, task 13 acc 0.5833333730697632, task 14 acc 0.9552083611488342, task 15 acc 0.800595223903656, task 16 acc 0.764266312122345, task 17 acc 0.8095238208770752, task 18 acc 0.682744562625885, task 19 acc 0.8570150136947632, task 20 acc 0.583251953125, task 21 acc 0.9498698115348816, task 22 acc 0.9177843451537182, task 23 acc 0.9034184169929131, task 24 acc 0.8959887623786926, task 25 acc 0.6359049677848816, task 26 acc 0.756944477558136, task 27 acc 0.5874837636947632, task 28 acc 0.808022677898407, task 29 acc 0.4593099057674408, task 30 acc 0.81103515625, task 31 acc 0.8425633311271667, task 32 acc 0.8084310293197632, task 33 acc 0.8434343338012695, task 34 acc 0.9391025900840759, task 35 acc 0.8253580927848816, task 36 acc 0.8822115659713745, task 37 acc 0.9798425436019897
train_loss 0.7618337757145055 1102000
train_loss 0.7420517143916804 1104000
train_loss 0.7475677798921242 1106000
train_loss 0.7359317577052862 1108000
train_loss 0.7438312544552609 1110000
train_loss 0.7242990934383124 1112000
train_loss 0.7344964809087106 1114000
train_loss 0.7611394374668599 1116000
train_loss 0.7441183998053893 1118000
train_loss 0.7632231548456475 1120000
train_loss 0.7673831809954718 1122000
train_loss 0.7374841907075607 1124000
train_loss 0.7504844867421779 1126000
train_loss 0.7605496046030894 1128000
train_loss 0.7274779119228478 1130000
train_loss 0.7403956304669845 1132000
train_loss 0.7348086052676662 1134000
train_loss 0.7514293433479033 1136000
train_loss 0.7479220945998095 1138000
train_loss 0.7321646548095159 1140000
train_loss 0.7251331961927936 1142000
train_loss 0.8447147706109099 1144000
train_loss 0.7573797774324194 1146000
train_loss 0.765907145183999 1148000
train_loss 0.7348157575377263 1150000
Validation loss 0.6687712245251803, avg acc 0.7492026090621948, task 0 acc 0.6819509267807007, task 1 acc 0.5604575276374817, task 2 acc 0.5902506709098816, task 3 acc 0.5141128897666931, task 4 acc 0.9160656929016113, task 5 acc 0.7323151230812073, task 6 acc 0.7588028311729431, task 7 acc 0.610107421875, task 8 acc 0.4561012089252472, task 9 acc 0.47592202512306225, task 10 acc 0.7536151659763549, task 11 acc 0.762499988079071, task 12 acc 0.7570097513527849, task 13 acc 0.59375, task 14 acc 0.9604167342185974, task 15 acc 0.8199405074119568, task 16 acc 0.754755437374115, task 17 acc 0.8229166865348816, task 18 acc 0.6881793737411499, task 19 acc 0.8614909052848816, task 20 acc 0.58349609375, task 21 acc 0.9498698115348816, task 22 acc 0.9192933871079301, task 23 acc 0.902485019856135, task 24 acc 0.8992537260055542, task 25 acc 0.6334635615348816, task 26 acc 0.7586805820465088, task 27 acc 0.5862630605697632, task 28 acc 0.8075658082962036, task 29 acc 0.43603515625, task 30 acc 0.8111165761947632, task 31 acc 0.8409810066223145, task 32 acc 0.8045247793197632, task 33 acc 0.8352272510528564, task 34 acc 0.9381009936332703, task 35 acc 0.8221029043197632, task 36 acc 0.889423131942749, task 37 acc 0.9811545610427856
train_loss 0.7477899675071239 1152000
train_loss 0.7699589464399033 1154000
train_loss 0.7366502909809352 1156000
train_loss 0.7419242799486965 1158000
train_loss 0.7376537057105452 1160000
train_loss 0.7653927584988997 1162000
train_loss 0.7743623980237171 1164000
train_loss 0.7654612333860714 1166000
train_loss 0.7294605585890822 1168000
train_loss 0.7315810241554864 1170000
train_loss 0.7280988879511133 1172000
train_loss 0.7473142721280456 1174000
train_loss 0.7399240200901404 1176000
train_loss 0.747840045384597 1178000
train_loss 0.7478229809443001 1180000
train_loss 0.7480440809065476 1182000
train_loss 0.7228625578135252 1184000
train_loss 0.7348080756482668 1186000
train_loss 0.7384058968522585 1188000
train_loss 0.7131458130020183 1190000
train_loss 0.7219765604489948 1192000
train_loss 0.7304059805788565 1194000
train_loss 0.7270195970204659 1196000
train_loss 0.7610205870992504 1198000
train_loss 0.7469069535133894 1200000
Validation loss 0.6679099331809399, avg acc 0.7480432391166687, task 0 acc 0.6842873692512512, task 1 acc 0.5502451062202454, task 2 acc 0.6251627802848816, task 3 acc 0.5141128897666931, task 4 acc 0.9034455418586731, task 5 acc 0.7139268517494202, task 6 acc 0.7658450603485107, task 7 acc 0.6136881709098816, task 8 acc 0.4613095223903656, task 9 acc 0.5007111494105773, task 10 acc 0.7476791186408822, task 11 acc 0.7606250047683716, task 12 acc 0.7634459696711384, task 13 acc 0.6041666865348816, task 14 acc 0.9572917222976685, task 15 acc 0.8139880895614624, task 16 acc 0.69972825050354, task 17 acc 0.8154761791229248, task 18 acc 0.698369562625885, task 19 acc 0.81103515625, task 20 acc 0.6199544668197632, task 21 acc 0.9536947011947632, task 22 acc 0.9175810876270587, task 23 acc 0.9065620868813581, task 24 acc 0.8997201323509216, task 25 acc 0.6261393427848816, task 26 acc 0.7361111044883728, task 27 acc 0.59619140625, task 28 acc 0.8073830604553223, task 29 acc 0.45947265625, task 30 acc 0.8059896230697632, task 31 acc 0.8409810066223145, task 32 acc 0.8028157949447632, task 33 acc 0.8248106241226196, task 34 acc 0.9379006624221802, task 35 acc 0.8196614980697632, task 36 acc 0.8858173489570618, task 37 acc 0.9803196787834167
train_loss 0.7295738797402009 1202000
train_loss 0.7313507951917126 1204000
train_loss 0.7526778469821438 1206000
train_loss 0.7502589068827219 1208000
train_loss 0.7627205902640708 1210000
train_loss 0.7229256651727483 1212000
train_loss 0.7305705498368479 1214000
train_loss 0.7444477140745148 1216000
train_loss 0.7249612509913277 1218000
train_loss 0.742046890472062 1220000
train_loss 0.7636285276077688 1222000
train_loss 0.7042648230926134 1224000
train_loss 0.7423470418653451 1226000
train_loss 0.7682378999611829 1228000
train_loss 0.7280020597614348 1230000
train_loss 0.7280473368330859 1232000
train_loss 0.7332273616560269 1234000
train_loss 0.7244331471871119 1236000
train_loss 0.7327504763873294 1238000
train_loss 0.744823173053097 1240000
train_loss 0.709211687752977 1242000
train_loss 0.7172308803033084 1244000
train_loss 0.7351316321194172 1246000
train_loss 0.7206645532681141 1248000
train_loss 0.7435026184022426 1250000
Validation loss 0.6408104176867642, avg acc 0.7546188831329346, task 0 acc 0.6813668012619019, task 1 acc 0.5586192607879639, task 2 acc 0.6607259511947632, task 3 acc 0.5, task 4 acc 0.9214743971824646, task 5 acc 0.7541197538375854, task 6 acc 0.7627640962600708, task 7 acc 0.62158203125, task 8 acc 0.470982164144516, task 9 acc 0.533187477746889, task 10 acc 0.7525645178835821, task 11 acc 0.7862499952316284, task 12 acc 0.7584366166314801, task 13 acc 0.5833333730697632, task 14 acc 0.9531250596046448, task 15 acc 0.7827380895614624, task 16 acc 0.7364130616188049, task 17 acc 0.8110119104385376, task 18 acc 0.6807065606117249, task 19 acc 0.8561198115348816, task 20 acc 0.6486002802848816, task 21 acc 0.95849609375, task 22 acc 0.9264361337897286, task 23 acc 0.9032686409074985, task 24 acc 0.9015858173370361, task 25 acc 0.6295573115348816, task 26 acc 0.7586805820465088, task 27 acc 0.5882161855697632, task 28 acc 0.808845043182373, task 29 acc 0.44970703125, task 30 acc 0.8111979365348816, task 31 acc 0.8425633311271667, task 32 acc 0.80419921875, task 33 acc 0.837436854839325, task 34 acc 0.9381009936332703, task 35 acc 0.82568359375, task 36 acc 0.895432710647583, task 37 acc 0.9819895029067993
train_loss 0.7227825266071595 1252000
train_loss 0.7223669858733192 1254000
train_loss 0.7156676632477902 1256000
train_loss 0.7232106638224796 1258000
train_loss 0.7250355463488959 1260000
train_loss 0.7473350740065798 1262000
train_loss 0.7155228719217702 1264000
train_loss 0.7215040492317639 1266000
train_loss 0.7100770680014975 1268000
train_loss 0.7294052619440481 1270000
train_loss 0.7291821771797259 1272000
train_loss 0.7334667043841909 1274000
train_loss 0.7048533363882452 1276000
train_loss 0.7260698455777019 1278000
train_loss 0.7382022087700898 1280000
train_loss 0.7092016586842946 1282000
train_loss 0.7351721224891953 1284000
train_loss 0.7322647142512724 1286000
train_loss 0.7168827762221918 1288000
train_loss 0.7164431802933104 1290000
train_loss 0.7303112287721597 1292000
train_loss 0.7212520998092369 1294000
train_loss 0.7214921719627455 1296000
train_loss 0.7288487174734474 1298000
train_loss 0.7100402480580378 1300000
Validation loss 0.6346392464594808, avg acc 0.7578224539756775, task 0 acc 0.6813668012619019, task 1 acc 0.5555555820465088, task 2 acc 0.68408203125, task 3 acc 0.5010080337524414, task 4 acc 0.9104567170143127, task 5 acc 0.7492966055870056, task 6 acc 0.7623239159584045, task 7 acc 0.6207682490348816, task 8 acc 0.4620535671710968, task 9 acc 0.5691982621949794, task 10 acc 0.7616445444912759, task 11 acc 0.8031249642372131, task 12 acc 0.744938500649651, task 13 acc 0.5833333730697632, task 14 acc 0.9583333730697632, task 15 acc 0.816964328289032, task 16 acc 0.77785325050354, task 17 acc 0.8065476417541504, task 18 acc 0.7004076242446899, task 19 acc 0.8588053584098816, task 20 acc 0.6743978261947632, task 21 acc 0.9529622793197632, task 22 acc 0.9187262537836676, task 23 acc 0.9041491499264837, task 24 acc 0.9043843150138855, task 25 acc 0.6299642324447632, task 26 acc 0.765625, task 27 acc 0.578857421875, task 28 acc 0.8015350699424744, task 29 acc 0.4462077021598816, task 30 acc 0.81396484375, task 31 acc 0.8366297483444214, task 32 acc 0.8013509511947632, task 33 acc 0.8339646458625793, task 34 acc 0.9350961446762085, task 35 acc 0.8230794668197632, task 36 acc 0.8870192766189575, task 37 acc 0.9812738299369812
train_loss 0.7084767769584432 1302000
train_loss 0.7149419376924634 1304000
train_loss 0.7135296630710364 1306000
train_loss 0.7111275799013674 1308000
train_loss 0.7184549267203547 1310000
train_loss 0.7319001692435704 1312000
train_loss 0.7021203793869354 1314000
train_loss 0.69197431294946 1316000
train_loss 0.6994958369981031 1318000
train_loss 0.6989880212531425 1320000
train_loss 0.6978948860915843 1322000
train_loss 0.696909324622713 1324000
train_loss 0.6980905352934497 1326000
train_loss 0.7509274824797176 1328000
train_loss 0.7519997755298391 1330000
train_loss 0.7340714388336055 1332000
train_loss 0.7356447099037469 1334000
train_loss 0.7101481675487011 1336000
train_loss 0.7291488301097415 1338000
train_loss 0.7159184161995072 1340000
train_loss 0.7162550862748176 1342000
train_loss 0.7342097983350977 1344000
train_loss 0.7370420436263084 1346000
train_loss 0.7336191382883117 1348000
train_loss 0.7241485776216723 1350000
Validation loss 0.6399768852642783, avg acc 0.7587428689002991, task 0 acc 0.6787382960319519, task 1 acc 0.5484068393707275, task 2 acc 0.694091796875, task 3 acc 0.49445563554763794, task 4 acc 0.9128605723381042, task 5 acc 0.7546221613883972, task 6 acc 0.7191901206970215, task 7 acc 0.61962890625, task 8 acc 0.4873512089252472, task 9 acc 0.5682458329022566, task 10 acc 0.7496465659319548, task 11 acc 0.8012499809265137, task 12 acc 0.773940796790495, task 13 acc 0.5833333730697632, task 14 acc 0.9541667103767395, task 15 acc 0.7708333730697632, task 16 acc 0.76902174949646, task 17 acc 0.8125, task 18 acc 0.707880437374115, task 19 acc 0.8590494990348816, task 20 acc 0.68359375, task 21 acc 0.9527181386947632, task 22 acc 0.9204565201298547, task 23 acc 0.9017689786943976, task 24 acc 0.9048507213592529, task 25 acc 0.63525390625, task 26 acc 0.7690972089767456, task 27 acc 0.5939127802848816, task 28 acc 0.8044590950012207, task 29 acc 0.4662272334098816, task 30 acc 0.8125814199447632, task 31 acc 0.8370253443717957, task 32 acc 0.8058268427848816, task 33 acc 0.8434343338012695, task 34 acc 0.9377003312110901, task 35 acc 0.8258463740348816, task 36 acc 0.8966346383094788, task 37 acc 0.9816316962242126
train_loss 0.7117929370831698 1352000
train_loss 0.7007104276712053 1354000
train_loss 0.7161223181793466 1356000
train_loss 0.7158288264193107 1358000
train_loss 0.7169682332728989 1360000
train_loss 0.7277927307398059 1362000
train_loss 0.735073336520698 1364000
train_loss 0.7066099405356218 1366000
train_loss 0.6982983211544342 1368000
train_loss 0.7151186119499616 1370000
train_loss 0.7541199015881866 1372000
train_loss 0.7189193620723672 1374000
train_loss 0.7258787603501696 1376000
train_loss 0.7373751497035846 1378000
train_loss 0.7095998559305444 1380000
train_loss 0.729604565308895 1382000
train_loss 0.7017861336641945 1384000
train_loss 0.7206946491566487 1386000
train_loss 0.7287747524492443 1388000
train_loss 0.7368573134238832 1390000
train_loss 0.7146382554974406 1392000
train_loss 0.7233115132995881 1394000
train_loss 0.7348366573113017 1396000
train_loss 0.7270598335266113 1398000
train_loss 0.7187591013102792 1400000
Validation loss 0.7195747533258962, avg acc 0.7532060742378235, task 0 acc 0.6664719581604004, task 1 acc 0.4867238700389862, task 2 acc 0.6954752802848816, task 3 acc 0.5050402879714966, task 4 acc 0.9184695482254028, task 5 acc 0.7559284567832947, task 6 acc 0.7495598196983337, task 7 acc 0.6163737177848816, task 8 acc 0.2485119104385376, task 9 acc 0.5675048670014455, task 10 acc 0.7491149678100248, task 11 acc 0.793749988079071, task 12 acc 0.7725598651321227, task 13 acc 0.59375, task 14 acc 0.9625000357627869, task 15 acc 0.8110119104385376, task 16 acc 0.7832880616188049, task 17 acc 0.8095238208770752, task 18 acc 0.6963315606117249, task 19 acc 0.8610026240348816, task 20 acc 0.6849772334098816, task 21 acc 0.953857421875, task 22 acc 0.9239421553708894, task 23 acc 0.90240965871043, task 24 acc 0.9090484976768494, task 25 acc 0.637939453125, task 26 acc 0.7552083134651184, task 27 acc 0.5869140625, task 28 acc 0.8125, task 29 acc 0.4659017026424408, task 30 acc 0.8131510615348816, task 31 acc 0.84375, task 32 acc 0.809814453125, task 33 acc 0.841856062412262, task 34 acc 0.9360977411270142, task 35 acc 0.8284505605697632, task 36 acc 0.8942307829856873, task 37 acc 0.9788883328437805
train_loss 0.7413813555329106 1402000
train_loss 0.714222685399931 1404000
train_loss 0.7137963827913627 1406000
train_loss 0.7101546886283905 1408000
train_loss 0.719603761786595 1410000
train_loss 0.6942813672586344 1412000
train_loss 0.7193670775424689 1414000
train_loss 0.7217243772149086 1416000
train_loss 0.7182721126845573 1418000
train_loss 0.6950188896008768 1420000
train_loss 0.7177689920454287 1422000
train_loss 0.7054687466868199 1424000
train_loss 0.7127462713397108 1426000
train_loss 0.7014424333367497 1428000
train_loss 0.6893105735355057 1430000
train_loss 0.6878531109117902 1432000
train_loss 0.7014477452479768 1434000
train_loss 0.7121553141162731 1436000
train_loss 0.711366931096185 1438000
train_loss 0.7132884385222569 1440000
train_loss 0.7172778089530766 1442000
train_loss 0.7205801333789714 1444000
train_loss 0.7144200188973918 1446000
train_loss 0.7334889740785584 1448000
train_loss 0.730828753628768 1450000
Validation loss 0.6386910072753246, avg acc 0.7593805193901062, task 0 acc 0.6831191182136536, task 1 acc 0.5565767884254456, task 2 acc 0.6939290761947632, task 3 acc 0.5045362710952759, task 4 acc 0.9178686141967773, task 5 acc 0.7622588276863098, task 6 acc 0.7491196990013123, task 7 acc 0.6229655146598816, task 8 acc 0.4947916865348816, task 9 acc 0.5303988966650856, task 10 acc 0.7663128449867543, task 11 acc 0.7949999570846558, task 12 acc 0.7677182174279394, task 13 acc 0.5833333730697632, task 14 acc 0.9583333730697632, task 15 acc 0.7797619104385376, task 16 acc 0.7629076242446899, task 17 acc 0.8214285969734192, task 18 acc 0.7038043737411499, task 19 acc 0.8609212636947632, task 20 acc 0.6852213740348816, task 21 acc 0.9545084834098816, task 22 acc 0.9308060143464437, task 23 acc 0.9102238138663903, task 24 acc 0.9057835340499878, task 25 acc 0.633544921875, task 26 acc 0.7517361044883728, task 27 acc 0.5869140625, task 28 acc 0.8064693212509155, task 29 acc 0.466064453125, task 30 acc 0.811279296875, task 31 acc 0.8322784900665283, task 32 acc 0.8056640625, task 33 acc 0.8424873948097229, task 34 acc 0.9362980723381042, task 35 acc 0.8231608271598816, task 36 acc 0.879807710647583, task 37 acc 0.9791269302368164
train_loss 0.7269176252996549 1452000
train_loss 0.7034150557271205 1454000
train_loss 0.7126768907159566 1456000
train_loss 0.7209051154020708 1458000
train_loss 0.710464733960107 1460000
train_loss 0.7117074141339399 1462000
train_loss 0.7081435575378128 1464000
train_loss 0.7162268438911997 1466000
train_loss 0.7274796630046331 1468000
train_loss 0.7233965064641088 1470000
train_loss 0.7433654237017036 1472000
train_loss 0.7408963406672702 1474000
train_loss 0.7242619761801324 1476000
train_loss 0.7185119323930703 1478000
train_loss 0.7474098652396352 1480000
train_loss 0.727957271238789 1482000
train_loss 0.7447492210627533 1484000
train_loss 0.7530997975887731 1486000
train_loss 0.7859314978988842 1488000
train_loss 0.7122405861862935 1490000
train_loss 0.7292640351708979 1492000
train_loss 0.7405573766683229 1494000
train_loss 0.7313042717329227 1496000
train_loss 0.7024891648925841 1498000
train_loss 0.7170229471870698 1500000
Validation loss 0.6446627433482851, avg acc 0.7592613101005554, task 0 acc 0.6819509267807007, task 1 acc 0.5592320561408997, task 2 acc 0.6905111074447632, task 3 acc 0.5045362710952759, task 4 acc 0.9144631624221802, task 5 acc 0.7415594458580017, task 6 acc 0.7513204216957092, task 7 acc 0.6175130605697632, task 8 acc 0.490327388048172, task 9 acc 0.5942036147578089, task 10 acc 0.7583608144549766, task 11 acc 0.7931249737739563, task 12 acc 0.7539329436322956, task 13 acc 0.5833333730697632, task 14 acc 0.9583333730697632, task 15 acc 0.8035714626312256, task 16 acc 0.7676630616188049, task 17 acc 0.8199405074119568, task 18 acc 0.6868206858634949, task 19 acc 0.861572265625, task 20 acc 0.6808268427848816, task 21 acc 0.9541015625, task 22 acc 0.921722941158798, task 23 acc 0.8960658781872464, task 24 acc 0.9048507213592529, task 25 acc 0.6279296875, task 26 acc 0.7586805820465088, task 27 acc 0.5758463740348816, task 28 acc 0.8083881735801697, task 29 acc 0.4520670771598816, task 30 acc 0.8142904043197632, task 31 acc 0.8457278609275818, task 32 acc 0.8091634511947632, task 33 acc 0.8409090638160706, task 34 acc 0.9375, task 35 acc 0.8238932490348816, task 36 acc 0.8870192766189575, task 37 acc 0.9806774854660034
train_loss 0.7380287416614592 1502000
train_loss 0.7052616131245159 1504000
train_loss 0.7469900906058028 1506000
train_loss 0.7044485514201224 1508000
train_loss 0.7374911436160327 1510000
train_loss 0.735615395723842 1512000
train_loss 0.7285964973710943 1514000
train_loss 0.7226288193636574 1516000
train_loss 0.7369482032624073 1518000
train_loss 0.7256295125130564 1520000
train_loss 0.7277547600469552 1522000
train_loss 0.7447742019267752 1524000
train_loss 0.72346130603645 1526000
train_loss 0.7119360173386522 1528000
train_loss 0.711178508033976 1530000
train_loss 0.7286442444946151 1532000
train_loss 0.7350868675741367 1534000
train_loss 0.7042427849727683 1536000
train_loss 0.761659140003845 1538000
train_loss 0.7070820937501267 1540000
train_loss 0.7162667236025445 1542000
train_loss 0.735535015927162 1544000
train_loss 0.6965218356803525 1546000
train_loss 0.7137713768128305 1548000
train_loss 0.7056241209732834 1550000
Validation loss 0.6472930382618364, avg acc 0.7554795145988464, task 0 acc 0.6752336025238037, task 1 acc 0.5520833134651184, task 2 acc 0.69873046875, task 3 acc 0.5105846524238586, task 4 acc 0.9126602411270142, task 5 acc 0.7394493222236633, task 6 acc 0.7526408433914185, task 7 acc 0.6163737177848816, task 8 acc 0.4546130895614624, task 9 acc 0.5205537258524096, task 10 acc 0.7416507440576674, task 11 acc 0.793749988079071, task 12 acc 0.7481192902331296, task 13 acc 0.5416666865348816, task 14 acc 0.9604167342185974, task 15 acc 0.7901785969734192, task 16 acc 0.776494562625885, task 17 acc 0.8184524178504944, task 18 acc 0.6739130616188049, task 19 acc 0.8570150136947632, task 20 acc 0.6837565302848816, task 21 acc 0.952880859375, task 22 acc 0.9253137422009655, task 23 acc 0.9006294452750206, task 24 acc 0.9113805890083313, task 25 acc 0.6357421875, task 26 acc 0.7586805820465088, task 27 acc 0.59765625, task 28 acc 0.8090277910232544, task 29 acc 0.4684244990348816, task 30 acc 0.8142904043197632, task 31 acc 0.8378164768218994, task 32 acc 0.8094075918197632, task 33 acc 0.8431186676025391, task 34 acc 0.9372996687889099, task 35 acc 0.826904296875, task 36 acc 0.8786057829856873, task 37 acc 0.9827051758766174
train_loss 0.722202577137854 1552000
train_loss 0.7179864063630812 1554000
train_loss 0.7168684881594963 1556000
train_loss 0.7260134235518053 1558000
train_loss 0.7218096012957395 1560000
train_loss 0.7184211708381772 1562000
train_loss 0.727830081328284 1564000
train_loss 0.7125003889929503 1566000
train_loss 0.7282072272403166 1568000
train_loss 0.7309777253065258 1570000
train_loss 0.7315376344639808 1572000
train_loss 0.7200740936130751 1574000
train_loss 0.71806864811806 1576000
train_loss 0.7032438582447358 1578000
train_loss 0.7018979976591654 1580000
train_loss 0.7163752131965011 1582000
train_loss 0.7152412375432905 1584000
train_loss 0.7183401827784255 1586000
train_loss 0.7144409933346324 1588000
train_loss 0.7003732743728905 1590000
train_loss 0.7212624782281928 1592000
train_loss 0.7238055020920001 1594000
train_loss 0.7350905964286066 1596000
train_loss 0.772352619712241 1598000
train_loss 0.7135190227515995 1600000
Validation loss 0.6442551320492295, avg acc 0.7565118670463562, task 0 acc 0.6799065470695496, task 1 acc 0.5578023195266724, task 2 acc 0.697509765625, task 3 acc 0.49697577953338623, task 4 acc 0.9156650900840759, task 5 acc 0.7516077160835266, task 6 acc 0.719630241394043, task 7 acc 0.6083984375, task 8 acc 0.4791666865348816, task 9 acc 0.5870923106718455, task 10 acc 0.736448847753612, task 11 acc 0.7987499833106995, task 12 acc 0.7536465429803374, task 13 acc 0.59375, task 14 acc 0.9572917222976685, task 15 acc 0.7559524178504944, task 16 acc 0.7520380616188049, task 17 acc 0.8154761791229248, task 18 acc 0.6773098111152649, task 19 acc 0.8631185293197632, task 20 acc 0.681640625, task 21 acc 0.963623046875, task 22 acc 0.9154314040055468, task 23 acc 0.9025235531737523, task 24 acc 0.9029850363731384, task 25 acc 0.63671875, task 26 acc 0.7743055820465088, task 27 acc 0.580322265625, task 28 acc 0.8103983998298645, task 29 acc 0.43896484375, task 30 acc 0.814208984375, task 31 acc 0.8445411324501038, task 32 acc 0.8092448115348816, task 33 acc 0.8409090638160706, task 34 acc 0.9383012652397156, task 35 acc 0.82666015625, task 36 acc 0.8882212042808533, task 37 acc 0.9809160232543945
train_loss 0.7099222968583927 1602000
train_loss 0.7146415278692729 1604000
train_loss 0.7282651732657105 1606000
train_loss 0.7187176920711063 1608000
train_loss 0.7351494943290018 1610000
train_loss 0.7131544911498204 1612000
train_loss 0.7392994639938697 1614000
train_loss 0.7219239768311382 1616000
train_loss 0.7055854382296093 1618000
train_loss 0.7157772574836854 1620000
train_loss 0.7013724178040865 1622000
train_loss 0.7110860862014815 1624000
train_loss 0.7320006919584703 1626000
train_loss 0.7002846118384041 1628000
train_loss 0.7052230995618738 1630000
train_loss 0.7290418853536248 1632000
train_loss 0.7136045027270448 1634000
train_loss 0.7070187270217575 1636000
train_loss 0.719109357347712 1638000
train_loss 0.730890363088809 1640000
train_loss 0.6963655309318565 1642000
train_loss 0.7163858684627339 1644000
train_loss 0.7378715704414062 1646000
train_loss 0.7267512449750211 1648000
train_loss 0.713222553900443 1650000
Validation loss 0.6423825028843039, avg acc 0.7616124749183655, task 0 acc 0.6822429895401001, task 1 acc 0.5586192607879639, task 2 acc 0.7089030146598816, task 3 acc 0.484375, task 4 acc 0.8878205418586731, task 5 acc 0.7616559267044067, task 6 acc 0.76408451795578, task 7 acc 0.6212565302848816, task 8 acc 0.4970238208770752, task 9 acc 0.5370371652058299, task 10 acc 0.7551176368312189, task 11 acc 0.8037499785423279, task 12 acc 0.7687649780048105, task 13 acc 0.5833333730697632, task 14 acc 0.9666666984558105, task 15 acc 0.7991071343421936, task 16 acc 0.7866848111152649, task 17 acc 0.8154761791229248, task 18 acc 0.6895380616188049, task 19 acc 0.86279296875, task 20 acc 0.6925455927848816, task 21 acc 0.9524739980697632, task 22 acc 0.927477558017434, task 23 acc 0.9038528333779636, task 24 acc 0.9118469953536987, task 25 acc 0.6389974355697632, task 26 acc 0.765625, task 27 acc 0.5865072011947632, task 28 acc 0.8099415302276611, task 29 acc 0.4716796875, task 30 acc 0.8148600459098816, task 31 acc 0.8390032052993774, task 32 acc 0.8089193105697632, task 33 acc 0.8484848737716675, task 34 acc 0.9379006624221802, task 35 acc 0.8260905146598816, task 36 acc 0.889423131942749, task 37 acc 0.9813931584358215
train_loss 0.7024031096510589 1652000
train_loss 0.6820351887629368 1654000
train_loss 0.6964599149525166 1656000
train_loss 0.6859511233193334 1658000
train_loss 0.6960419310261495 1660000
train_loss 0.6831443286938593 1662000
train_loss 0.6794774305578322 1664000
train_loss 0.6815616543354699 1666000
train_loss 0.678045341979363 1668000
train_loss 0.6748226369367912 1670000
train_loss 0.6940060944426805 1672000
train_loss 0.6677960492586718 1674000
train_loss 0.7045688781351782 1676000
train_loss 0.6864789334477391 1678000
train_loss 0.6799411493409425 1680000
train_loss 0.6732646432181355 1682000
train_loss 0.679192424160894 1684000
train_loss 0.6744928900739178 1686000
train_loss 0.6755363174844533 1688000
train_loss 0.6900245272512547 1690000
train_loss 0.6602345707556233 1692000
train_loss 0.6651893548169173 1694000
train_loss 0.6735771816160996 1696000
train_loss 0.6567855816676748 1698000
train_loss 0.6687719757712912 1700000
Validation loss 0.6208164163000668, avg acc 0.7664650678634644, task 0 acc 0.6842873692512512, task 1 acc 0.5567810535430908, task 2 acc 0.710693359375, task 3 acc 0.508568525314331, task 4 acc 0.9166666865348816, task 5 acc 0.7644694447517395, task 6 acc 0.7711267471313477, task 7 acc 0.6216634511947632, task 8 acc 0.527529776096344, task 9 acc 0.555235488020903, task 10 acc 0.7549445312535068, task 11 acc 0.809374988079071, task 12 acc 0.7713912477001506, task 13 acc 0.5833333730697632, task 14 acc 0.9635417461395264, task 15 acc 0.8139880895614624, task 16 acc 0.779891312122345, task 17 acc 0.8303571343421936, task 18 acc 0.70652174949646, task 19 acc 0.8643392324447632, task 20 acc 0.701904296875, task 21 acc 0.96728515625, task 22 acc 0.9302135820233223, task 23 acc 0.9080577013840991, task 24 acc 0.90625, task 25 acc 0.6375325918197632, task 26 acc 0.7638888955116272, task 27 acc 0.5990397334098816, task 28 acc 0.8124086260795593, task 29 acc 0.4563802182674408, task 30 acc 0.8152669668197632, task 31 acc 0.8445411324501038, task 32 acc 0.8089193105697632, task 33 acc 0.8424873948097229, task 34 acc 0.9417067170143127, task 35 acc 0.8248698115348816, task 36 acc 0.8882212042808533, task 37 acc 0.9819895029067993
train_loss 0.642974934132304 1702000
train_loss 0.6705550213025417 1704000
train_loss 0.6809667534674518 1706000
train_loss 0.6788093557489338 1708000
train_loss 0.6789747812554706 1710000
train_loss 0.6747691338793375 1712000
train_loss 0.6446208364665508 1714000
train_loss 0.6696101309340448 1716000
train_loss 0.6593003417539876 1718000
train_loss 0.6844337341093923 1720000
train_loss 0.6823528681045864 1722000
train_loss 0.6642224998797756 1724000
train_loss 0.6664530590141657 1726000
train_loss 0.6708440035993699 1728000
train_loss 0.6712388574287761 1730000
train_loss 0.6649917746258434 1732000
train_loss 0.6667891091707862 1734000
train_loss 0.6634137906408869 1736000
train_loss 0.6660723224633839 1738000
train_loss 0.653538157814648 1740000
train_loss 0.6500176023293752 1742000
train_loss 0.6648629538691603 1744000
train_loss 0.6774509943696903 1746000
train_loss 0.6540464247115888 1748000
train_loss 0.6652410250026732 1750000
Validation loss 0.61125661030391, avg acc 0.7696552276611328, task 0 acc 0.6764018535614014, task 1 acc 0.5490196347236633, task 2 acc 0.7058919668197632, task 3 acc 0.5045362710952759, task 4 acc 0.9152644276618958, task 5 acc 0.7643689513206482, task 6 acc 0.76716548204422, task 7 acc 0.62890625, task 8 acc 0.5446428656578064, task 9 acc 0.6046142271486206, task 10 acc 0.7680510104023186, task 11 acc 0.8206250071525574, task 12 acc 0.7729352277354296, task 13 acc 0.5729166865348816, task 14 acc 0.9614583849906921, task 15 acc 0.816964328289032, task 16 acc 0.78736412525177, task 17 acc 0.8303571343421936, task 18 acc 0.7255434989929199, task 19 acc 0.8631185293197632, task 20 acc 0.6881510615348816, task 21 acc 0.9669596552848816, task 22 acc 0.9333509747443511, task 23 acc 0.9089187219427791, task 24 acc 0.9076492190361023, task 25 acc 0.6443685293197632, task 26 acc 0.7795138955116272, task 27 acc 0.5909830927848816, task 28 acc 0.8089364171028137, task 29 acc 0.4808756709098816, task 30 acc 0.8143717646598816, task 31 acc 0.8457278609275818, task 32 acc 0.8090006709098816, task 33 acc 0.8431186676025391, task 34 acc 0.9411057829856873, task 35 acc 0.827392578125, task 36 acc 0.8930288553237915, task 37 acc 0.9833015203475952
train_loss 0.6806979308221489 1752000
train_loss 0.6603675687117502 1754000
train_loss 0.6607689982561861 1756000
train_loss 0.6649617517534644 1758000
train_loss 0.6551806485033594 1760000
train_loss 0.6768965941998176 1762000
train_loss 0.6536902459864504 1764000
train_loss 0.6591686900589848 1766000
train_loss 0.6650409825376701 1768000
train_loss 0.6551531333471649 1770000
train_loss 0.6864077918976546 1772000
train_loss 0.6634889360696543 1774000
train_loss 0.665672002792824 1776000
train_loss 0.6599363617631607 1778000
train_loss 0.6737728582352865 1780000
train_loss 0.6624846800246742 1782000
train_loss 0.6641059744888916 1784000
train_loss 0.6694688816936687 1786000
train_loss 0.6487458125851117 1788000
train_loss 0.665226165197324 1790000
train_loss 0.6516440503792837 1792000
train_loss 0.660996564398054 1794000
train_loss 0.6592027759233023 1796000
train_loss 0.6702957435888238 1798000
train_loss 0.6639048769213259 1800000
Validation loss 0.6321227690783541, avg acc 0.7651010155677795, task 0 acc 0.6755256652832031, task 1 acc 0.5645424723625183, task 2 acc 0.7146810293197632, task 3 acc 0.508568525314331, task 4 acc 0.9246795177459717, task 5 acc 0.7705988883972168, task 6 acc 0.7645246386528015, task 7 acc 0.6285807490348816, task 8 acc 0.5334821343421936, task 9 acc 0.423596680165475, task 10 acc 0.7624592346211235, task 11 acc 0.8243749737739563, task 12 acc 0.7644361941646012, task 13 acc 0.5833333730697632, task 14 acc 0.9604167342185974, task 15 acc 0.8065476417541504, task 16 acc 0.80910325050354, task 17 acc 0.8184524178504944, task 18 acc 0.7167119979858398, task 19 acc 0.8640950918197632, task 20 acc 0.6963704824447632, task 21 acc 0.968505859375, task 22 acc 0.9268161616897873, task 23 acc 0.9066987359685957, task 24 acc 0.902518630027771, task 25 acc 0.64404296875, task 26 acc 0.7743055820465088, task 27 acc 0.5992025136947632, task 28 acc 0.8103070259094238, task 29 acc 0.4833170771598816, task 30 acc 0.8157552480697632, task 31 acc 0.8354430794715881, task 32 acc 0.807373046875, task 33 acc 0.846275269985199, task 34 acc 0.9411057829856873, task 35 acc 0.825439453125, task 36 acc 0.889423131942749, task 37 acc 0.9822280406951904
train_loss 0.6533501837821677 1802000
train_loss 0.6632076774497982 1804000
train_loss 0.6416012865970843 1806000
train_loss 0.6600667519068811 1808000
train_loss 0.6594314163312083 1810000
train_loss 0.6572597374306061 1812000
train_loss 0.6694896866411436 1814000
train_loss 0.6538618252283195 1816000
train_loss 0.6743555413139984 1818000
train_loss 0.6438156926550437 1820000
train_loss 0.647273216615431 1822000
train_loss 0.6515617120908573 1824000
train_loss 0.6670962888458744 1826000
train_loss 0.661306159142172 1828000
train_loss 0.6429578394677956 1830000
train_loss 0.650221852008719 1832000
train_loss 0.6635811673651915 1834000
train_loss 0.6511787607746664 1836000
train_loss 0.6620314518553205 1838000
train_loss 0.6557799413632601 1840000
train_loss 0.6746699915314093 1842000
train_loss 0.6452521288320422 1844000
train_loss 0.6497164752812823 1846000
train_loss 0.6517291032557841 1848000
train_loss 0.6734481764408993 1850000
Validation loss 0.6074179208222559, avg acc 0.7695298790931702, task 0 acc 0.6787382960319519, task 1 acc 0.5596405267715454, task 2 acc 0.7159017324447632, task 3 acc 0.49042338132858276, task 4 acc 0.9244791865348816, task 5 acc 0.7638665437698364, task 6 acc 0.7750880122184753, task 7 acc 0.6289876699447632, task 8 acc 0.5386905074119568, task 9 acc 0.5973382809111457, task 10 acc 0.752891947998937, task 11 acc 0.8293749690055847, task 12 acc 0.7845220866073664, task 13 acc 0.5833333730697632, task 14 acc 0.9656250476837158, task 15 acc 0.7872024178504944, task 16 acc 0.8179348111152649, task 17 acc 0.8229166865348816, task 18 acc 0.6834239363670349, task 19 acc 0.861572265625, task 20 acc 0.698974609375, task 21 acc 0.9698079824447632, task 22 acc 0.9299132390703424, task 23 acc 0.9081192167531899, task 24 acc 0.9141790866851807, task 25 acc 0.6451823115348816, task 26 acc 0.7621527910232544, task 27 acc 0.5967611074447632, task 28 acc 0.8116776347160339, task 29 acc 0.4849446713924408, task 30 acc 0.813232421875, task 31 acc 0.846123456954956, task 32 acc 0.809814453125, task 33 acc 0.8424873948097229, task 34 acc 0.9405048489570618, task 35 acc 0.8280436396598816, task 36 acc 0.8966346383094788, task 37 acc 0.9816316962242126
train_loss 0.6626286506126635 1852000
train_loss 0.6534064605613239 1854000
train_loss 0.6479508366344963 1856000
train_loss 0.6534818598176353 1858000
train_loss 0.6546812655278481 1860000
train_loss 0.6581335409719031 1862000
train_loss 0.6419541007138323 1864000
train_loss 0.6433391875319648 1866000
train_loss 0.6521252763129305 1868000
train_loss 0.6528057752493769 1870000
train_loss 0.6444738329952816 1872000
train_loss 0.6528638955885544 1874000
train_loss 0.6540181611031294 1876000
train_loss 0.6390337889725343 1878000
train_loss 0.6646596036688425 1880000
train_loss 0.6497512430555653 1882000
train_loss 0.6478934197765776 1884000
train_loss 0.6444507656865753 1886000
train_loss 0.6463056484465487 1888000
train_loss 0.6426365171193611 1890000
train_loss 0.6485633864183911 1892000
train_loss 0.6426110706039472 1894000
train_loss 0.6543649341680575 1896000
train_loss 0.6521844734186306 1898000
train_loss 0.6576726322839967 1900000
